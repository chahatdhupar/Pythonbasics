{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8kMscRfQGmpo"
      },
      "source": [
        "Question 1: What is a Convolutional Neural Network (CNN), and how does it differ from\n",
        "traditional fully connected neural networks in terms of architecture and performance on\n",
        "image data?\n",
        "\n",
        "\n",
        "A Convolutional Neural Network (CNN) is a type of deep neural network designed for processing grid-like data, such as images, by applying convolutional operations to extract spatial features. It consists of convolutional layers that use filters (kernels) to detect patterns like edges or textures, followed by pooling layers to reduce dimensionality, and fully connected layers for classification.\n",
        "\n",
        "In contrast, traditional fully connected neural networks (e.g., multi-layer perceptrons) connect every neuron in one layer to every neuron in the next, treating input as a flat vector without spatial structure. This makes them less efficient for images, as they ignore local dependencies and require many parameters.\n",
        "\n",
        "Architectural Differences:\n",
        "\n",
        "CNNs: Use shared weights in convolutions for translation invariance, include pooling for downsampling, and have fewer parameters due to weight sharing.\n",
        "Fully Connected NNs: Dense connections lead to high parameter counts and risk overfitting on high-dimensional data.\n",
        "Performance on Image Data:\n",
        "\n",
        "CNNs excel at image tasks by capturing hierarchical features (e.g., edges to objects), achieving better accuracy with less data and computation. Fully connected NNs struggle with large images due to parameter explosion and lack of spatial awareness, often underperforming unless data is preprocessed.\n",
        "\n",
        "\n",
        "\n",
        "Question 2: Discuss the architecture of LeNet-5 and explain how it laid the foundation\n",
        "for modern deep learning models in computer vision. Include references to its original\n",
        "research paper\n",
        "\n",
        "\n",
        "LeNet-5, introduced by Yann LeCun et al. in 1998, is a pioneering CNN for handwritten digit recognition. Its architecture includes:\n",
        "\n",
        "Input Layer: 32x32 grayscale images.\n",
        "Convolutional Layers: Two conv layers (C1: 6 filters of 5x5, C3: 16 filters of 5x5) with tanh activation.\n",
        "Pooling Layers: Two subsampling layers (S2, S4) using average pooling (2x2) to reduce size.\n",
        "Fully Connected Layers: Two dense layers (F5: 120 neurons, F6: 84 neurons) followed by a 10-class output with softmax.\n",
        "Total Parameters: ~60,000, with connections that mimic biological vision.\n",
        "It laid the foundation for modern computer vision by demonstrating end-to-end learning from raw pixels, introducing convolutional and pooling layers for feature extraction, and enabling scalable training on GPUs. This influenced models like AlexNet and ResNet by establishing CNNs as standard for vision tasks.\n",
        "\n",
        "Reference: LeCun, Y., Bottou, L., Bengio, Y., & Haffner, P. (1998). Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11), 2278-2324.\n",
        "\n",
        "\n",
        "\n",
        "Question 3: Compare and contrast AlexNet and VGGNet in terms of design principles,\n",
        "number of parameters, and performance. Highlight key innovations and limitations of\n",
        "each.\n",
        "\n",
        "\n",
        "Design Principles:\n",
        "\n",
        "AlexNet: Uses a deeper architecture (8 layers) with large filters (11x11 in first layer), ReLU activation, dropout for regularization, and local response normalization. It splits computation across GPUs.\n",
        "VGGNet: Emphasizes simplicity with small 3x3 filters stacked deeply (up to 19 layers in VGG19), increasing depth while keeping parameters manageable through uniform filter sizes.\n",
        "Number of Parameters:\n",
        "\n",
        "AlexNet: ~60 million parameters.\n",
        "VGGNet: VGG16 has ~138 million, VGG19 has ~144 million—higher due to depth, but efficient per layer.\n",
        "Performance:\n",
        "\n",
        "AlexNet: Won ImageNet 2012 with 15.3% top-5 error, revolutionizing deep learning by proving depth's value.\n",
        "VGGNet: Achieved 6.8% top-5 error on ImageNet, better generalization but slower training due to depth.\n",
        "Key Innovations and Limitations:\n",
        "\n",
        "AlexNet Innovations: ReLU for faster training, dropout to prevent overfitting. Limitations: High parameters, GPU-specific design.\n",
        "VGGNet Innovations: Demonstrated that deeper networks with small filters improve accuracy. Limitations: Computationally intensive, prone to overfitting without regularization.\n",
        "\n",
        "\n",
        "\n",
        "Question 4: What is transfer learning in the context of image classification? Explain\n",
        "how it helps in reducing computational costs and improving model performance with\n",
        "limited data.\n",
        "\n",
        "Transfer learning involves using a pre-trained model (trained on a large dataset like ImageNet) and adapting it to a new task with limited data. For image classification, it leverages learned features (e.g., edges, shapes) from the source domain.\n",
        "\n",
        "Reducing Computational Costs: Instead of training from scratch, freeze early layers and fine-tune later ones, saving time and resources—e.g., training a full model might take days, but fine-tuning takes hours.\n",
        "\n",
        "Improving Performance with Limited Data: Pre-trained weights provide a strong starting point, reducing overfitting. For example, on a small dataset, accuracy can improve by 10-20% compared to random initialization, as the model generalizes better from learned representations.\n",
        "\n",
        "\n",
        "Question 5: Describe the role of residual connections in ResNet architecture. How do\n",
        "they address the vanishing gradient problem in deep CNNs?\n",
        "\n",
        "Residual connections in ResNet (He et al., 2016) add skip connections that bypass one or more layers, allowing the input to be added directly to the output: $ y = F(x) + x $, where $ F(x) $ is the residual mapping.\n",
        "\n",
        "They address the vanishing gradient problem in deep CNNs by enabling gradients to flow directly through shortcuts, preventing them from diminishing in very deep networks. This allows training of networks with hundreds of layers (e.g., ResNet-152), improving accuracy on tasks like ImageNet without degradation.\n",
        "\n",
        "\n",
        "Question 6: Implement the LeNet-5 architectures using Tensorflow or PyTorch to\n",
        "classify the MNIST dataset. Report the accuracy and training time.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NjAktCtdILvU"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "import time\n",
        "\n",
        "# LeNet-5 Architecture\n",
        "class LeNet5(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNet5, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
        "        self.pool = nn.AvgPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16*5*5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(torch.tanh(self.conv1(x)))\n",
        "        x = self.pool(torch.tanh(self.conv2(x)))\n",
        "        x = x.view(-1, 16*5*5)\n",
        "        x = torch.tanh(self.fc1(x))\n",
        "        x = torch.tanh(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Data Loading\n",
        "transform = transforms.Compose([transforms.Resize((32, 32)), transforms.ToTensor()])\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# Model, Loss, Optimizer\n",
        "model = LeNet5()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training\n",
        "start_time = time.time()\n",
        "for epoch in range(10):\n",
        "    for images, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "training_time = time.time() - start_time\n",
        "\n",
        "# Evaluation\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "accuracy = 100 * correct / total\n",
        "\n",
        "print(f\"Training Time: {training_time:.2f} seconds\")\n",
        "print(f\"Test Accuracy: {accuracy:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jE0XwIBgIPpa"
      },
      "source": [
        "Question 7: Use a pre-trained VGG16 model (via transfer learning) on a small custom\n",
        "dataset (e.g., flowers or animals). Replace the top layers and fine-tune the model.\n",
        "Include your code and result discussion."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tPeQmy5sIQSu"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import models, transforms, datasets\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import time\n",
        "import os\n",
        "import tarfile\n",
        "import urllib.request\n",
        "\n",
        "# 1. Download and extract the dataset\n",
        "dataset_url = \"https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\"\n",
        "dataset_path = './flower_photos.tgz'\n",
        "extract_path = './flower_photos'\n",
        "\n",
        "if not os.path.exists(extract_path):\n",
        "    print(f\"Downloading dataset from {dataset_url}...\")\n",
        "    urllib.request.urlretrieve(dataset_url, dataset_path)\n",
        "    print(\"Extracting dataset...\")\n",
        "    with tarfile.open(dataset_path, 'r:gz') as tar:\n",
        "        tar.extractall(path='./')\n",
        "    print(\"Dataset extracted.\")\n",
        "    os.remove(dataset_path) # Clean up the tgz file\n",
        "else:\n",
        "    print(\"Dataset already exists, skipping download.\")\n",
        "\n",
        "# Adjust the root directory for ImageFolder to point to the extracted dataset\n",
        "data_dir = extract_path\n",
        "\n",
        "# Assume custom dataset in 'data/flowers' with subfolders per class\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Load the full dataset and then split it into training and testing\n",
        "full_dataset = datasets.ImageFolder(root=data_dir, transform=transform)\n",
        "\n",
        "# Determine the number of classes dynamically\n",
        "num_classes = len(full_dataset.classes)\n",
        "\n",
        "# Split dataset into train and test sets (e.g., 80% train, 20% test)\n",
        "train_size = int(0.8 * len(full_dataset))\n",
        "test_size = len(full_dataset) - train_size\n",
        "train_dataset, test_dataset = random_split(full_dataset, [train_size, test_size])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "# Load pre-trained VGG16\n",
        "model = models.vgg16(pretrained=True)\n",
        "# Freeze feature layers\n",
        "for param in model.features.parameters():\n",
        "    param.requires_grad = False\n",
        "# Replace classifier\n",
        "# model.classifier[6] = nn.Linear(4096, 5) # Original code assumed 5 classes\n",
        "model.classifier[6] = nn.Linear(4096, num_classes)  # Dynamically set num_classes\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.classifier.parameters(), lr=0.001)\n",
        "\n",
        "# Fine-tuning\n",
        "start_time = time.time()\n",
        "epochs = 5 # Reduced for quicker execution\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "    epoch_loss = running_loss / len(train_dataset)\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss:.4f}\")\n",
        "training_time = time.time() - start_time\n",
        "\n",
        "# Evaluation\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "accuracy = 100 * correct / total\n",
        "\n",
        "print(f\"Training Time: {training_time:.2f} seconds\")\n",
        "print(f\"Test Accuracy: {accuracy:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_an3pO7WHAv8"
      },
      "source": [
        "Question 8: Write a program to visualize the filters and feature maps of the first\n",
        "convolutional layer of AlexNet on an example input image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W_GGnPMaHBMm"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import requests\n",
        "\n",
        "# Load pre-trained AlexNet\n",
        "model = models.alexnet(pretrained=True)\n",
        "model.eval()\n",
        "\n",
        "# Get the first convolutional layer\n",
        "first_conv = model.features[0]  # Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
        "\n",
        "# Extract filters (weights) from the first conv layer\n",
        "filters = first_conv.weight.data.cpu().numpy()  # Shape: (64, 3, 11, 11)\n",
        "\n",
        "# Load an example image (e.g., a cat image from ImageNet or any RGB image)\n",
        "# For demonstration, we'll use a sample image; replace with your own path\n",
        "\n",
        "# Download a sample image if it doesn't exist\n",
        "image_path = 'sample_image.jpg'\n",
        "if not os.path.exists(image_path):\n",
        "    print(f\"Downloading sample image to {image_path}...\")\n",
        "    # Using an alternative publicly accessible image URL\n",
        "    # Replaced the previous URL with another one from a reliable source\n",
        "    image_url = \"https://www.w3.org/People/Raggett/emma/emma.jpg\" # A very stable sample image from W3C\n",
        "    try:\n",
        "        response = requests.get(image_url, stream=True)\n",
        "        response.raise_for_status() # Raise an exception for HTTP errors\n",
        "        with open(image_path, 'wb') as f:\n",
        "            for chunk in response.iter_content(chunk_size=8192):\n",
        "                f.write(chunk)\n",
        "        print(\"Download complete.\")\n",
        "    except requests.exceptions.HTTPError as e:\n",
        "        print(f\"Error downloading image: {e}\")\n",
        "        print(\"Please provide a valid image URL or ensure the image_path exists locally.\")\n",
        "        # Fallback if download fails: create a dummy image or exit\n",
        "        exit() # Exit to prevent further errors if image download fails\n",
        "\n",
        "image = Image.open(image_path).convert('RGB')\n",
        "\n",
        "# Preprocess the image as AlexNet expects (224x224, normalized)\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "input_tensor = preprocess(image).unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "# Pass through the first conv layer to get feature maps\n",
        "with torch.no_grad():\n",
        "    feature_maps = first_conv(input_tensor).cpu().numpy()[0]  # Shape: (64, 55, 55) before pooling, but AlexNet has pooling next\n",
        "\n",
        "# Note: AlexNet's first conv output is 55x55x64, then pooled to 27x27x64. We'll visualize the conv output.\n",
        "\n",
        "# Visualize filters\n",
        "fig, axes = plt.subplots(8, 8, figsize=(12, 12))  # 8x8 grid for 64 filters\n",
        "for i in range(64):\n",
        "    ax = axes[i // 8, i % 8]\n",
        "    # Normalize filter for visualization (each filter is 3x11x11)\n",
        "    filter_img = filters[i].transpose(1, 2, 0)  # (11, 11, 3)\n",
        "    # Ensure float type for imshow, otherwise it might clip values or interpret as indices\n",
        "    filter_img = (filter_img - filter_img.min()) / (filter_img.max() - filter_img.min()) # Normalize to [0,1]\n",
        "    ax.imshow(filter_img)\n",
        "    ax.axis('off')\n",
        "plt.suptitle('Filters of First Conv Layer (AlexNet)')\n",
        "plt.show()\n",
        "\n",
        "# Visualize feature maps\n",
        "fig, axes = plt.subplots(8, 8, figsize=(12, 12))  # 8x8 grid for 64 feature maps\n",
        "for i in range(64):\n",
        "    ax = axes[i // 8, i % 8]\n",
        "    feature_map = feature_maps[i]\n",
        "    # Normalize for visualization\n",
        "    feature_map = (feature_map - feature_map.min()) / (feature_map.max() - feature_map.min() + 1e-8) # Add epsilon to avoid division by zero\n",
        "    ax.imshow(feature_map, cmap='gray')\n",
        "    ax.axis('off')\n",
        "plt.suptitle('Feature Maps of First Conv Layer (AlexNet)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_B4v4gpzG89P"
      },
      "source": [
        "Question 9: Train a GoogLeNet (Inception v1) or its variant using a standard dataset\n",
        "like CIFAR-10. Plot the training and validation accuracy over epochs and analyze\n",
        "overfitting or underfitting.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "u7PoFbZYG9_Z",
        "outputId": "4d809577-d0c4-4309-b7ad-beac49b4ef69"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 170M/170M [00:03<00:00, 45.5MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20, Train Acc: 16.13%, Val Acc: 18.77%\n",
            "Epoch 2/20, Train Acc: 26.26%, Val Acc: 32.07%\n",
            "Epoch 3/20, Train Acc: 35.96%, Val Acc: 39.58%\n",
            "Epoch 4/20, Train Acc: 41.15%, Val Acc: 44.67%\n",
            "Epoch 5/20, Train Acc: 44.84%, Val Acc: 47.50%\n",
            "Epoch 6/20, Train Acc: 47.58%, Val Acc: 47.67%\n",
            "Epoch 7/20, Train Acc: 50.03%, Val Acc: 50.53%\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define a simplified Inception module (GoogLeNet variant for CIFAR-10)\n",
        "class Inception(nn.Module):\n",
        "    def __init__(self, in_channels, ch1x1, ch3x3red, ch3x3, ch5x5red, ch5x5, pool_proj):\n",
        "        super(Inception, self).__init__()\n",
        "        self.branch1 = nn.Conv2d(in_channels, ch1x1, kernel_size=1)\n",
        "\n",
        "        self.branch2 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, ch3x3red, kernel_size=1),\n",
        "            nn.Conv2d(ch3x3red, ch3x3, kernel_size=3, padding=1)\n",
        "        )\n",
        "\n",
        "        self.branch3 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, ch5x5red, kernel_size=1),\n",
        "            nn.Conv2d(ch5x5red, ch5x5, kernel_size=5, padding=2)\n",
        "        )\n",
        "\n",
        "        self.branch4 = nn.Sequential(\n",
        "            nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\n",
        "            nn.Conv2d(in_channels, pool_proj, kernel_size=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        branch1 = self.branch1(x)\n",
        "        branch2 = self.branch2(x)\n",
        "        branch3 = self.branch3(x)\n",
        "        branch4 = self.branch4(x)\n",
        "        outputs = [branch1, branch2, branch3, branch4]\n",
        "        return torch.cat(outputs, 1)\n",
        "\n",
        "class GoogLeNetVariant(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(GoogLeNetVariant, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3)\n",
        "        self.maxpool1 = nn.MaxPool2d(3, stride=2, padding=1)\n",
        "        self.conv2 = nn.Conv2d(64, 64, kernel_size=1)\n",
        "        self.conv3 = nn.Conv2d(64, 192, kernel_size=3, padding=1)\n",
        "        self.maxpool2 = nn.MaxPool2d(3, stride=2, padding=1)\n",
        "\n",
        "        self.inception3a = Inception(192, 64, 96, 128, 16, 32, 32)\n",
        "        self.inception3b = Inception(256, 128, 128, 192, 32, 96, 64)\n",
        "        self.maxpool3 = nn.MaxPool2d(3, stride=2, padding=1)\n",
        "\n",
        "        self.inception4a = Inception(480, 192, 96, 208, 16, 48, 64)\n",
        "        self.inception4b = Inception(512, 160, 112, 224, 24, 64, 64)\n",
        "        self.inception4c = Inception(512, 128, 128, 256, 24, 64, 64)\n",
        "        self.inception4d = Inception(512, 112, 144, 288, 32, 64, 64)\n",
        "        self.inception4e = Inception(528, 256, 160, 320, 32, 128, 128)\n",
        "        self.maxpool4 = nn.MaxPool2d(3, stride=2, padding=1)\n",
        "\n",
        "        self.inception5a = Inception(832, 256, 160, 320, 32, 128, 128)\n",
        "        self.inception5b = Inception(832, 384, 192, 384, 48, 128, 128)\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.dropout = nn.Dropout(0.4)\n",
        "        self.fc = nn.Linear(1024, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.maxpool1(torch.relu(self.conv1(x)))\n",
        "        x = torch.relu(self.conv2(x))\n",
        "        x = self.maxpool2(torch.relu(self.conv3(x)))\n",
        "\n",
        "        x = self.inception3a(x)\n",
        "        x = self.inception3b(x)\n",
        "        x = self.maxpool3(x)\n",
        "\n",
        "        x = self.inception4a(x)\n",
        "        x = self.inception4b(x)\n",
        "        x = self.inception4c(x)\n",
        "        x = self.inception4d(x)\n",
        "        x = self.inception4e(x)\n",
        "        x = self.maxpool4(x)\n",
        "\n",
        "        x = self.inception5a(x)\n",
        "        x = self.inception5b(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "# Data loading\n",
        "transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
        "\n",
        "# Model, loss, optimizer\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = GoogLeNetVariant().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training\n",
        "num_epochs = 20\n",
        "train_accuracies = []\n",
        "val_accuracies = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total_train += labels.size(0)\n",
        "        correct_train += (predicted == labels).sum().item()\n",
        "\n",
        "    train_acc = 100 * correct_train / total_train\n",
        "    train_accuracies.append(train_acc)\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    correct_val = 0\n",
        "    total_val = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total_val += labels.size(0)\n",
        "            correct_val += (predicted == labels).sum().item()\n",
        "\n",
        "    val_acc = 100 * correct_val / total_val\n",
        "    val_accuracies.append(val_acc)\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, Train Acc: {train_acc:.2f}%, Val Acc: {val_acc:.2f}%')\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(range(1, num_epochs+1), train_accuracies, label='Training Accuracy')\n",
        "plt.plot(range(1, num_epochs+1), val_accuracies, label='Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.title('Training and Validation Accuracy over Epochs')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QsoKVcrZHHQH"
      },
      "source": [
        "Question 10: You are working in a healthcare AI startup. Your team is tasked with\n",
        "developing a system that automatically classifies medical X-ray images into normal,\n",
        "pneumonia, and COVID-19. Due to limited labeled data, what approach would you\n",
        "suggest using among CNN architectures discussed (e.g., transfer learning with ResNet\n",
        "or Inception variants)? Justify your approach and outline a deployment strategy for\n",
        "production use.\n",
        "\n",
        "\n",
        "Given the limited labeled data for medical X-ray classification (normal, pneumonia, and COVID-19), I recommend using transfer learning with a pre-trained ResNet model (e.g., ResNet-50 or ResNet-101). This leverages the CNN architectures we've discussed, such as ResNet's residual connections for handling deep networks and mitigating vanishing gradients, while adapting to the task efficiently.\n",
        "\n",
        "Justification\n",
        "Handling Limited Data: Transfer learning allows us to use a model pre-trained on a large dataset like ImageNet, which has learned general features (e.g., edges, textures) applicable to X-rays. We freeze the early layers (which capture low-level features) and fine-tune the later layers on our small dataset. This reduces overfitting, as the model doesn't start from random weights, and can achieve high performance with as few as 1,000-5,000 labeled images per class (common in medical scenarios).\n",
        "Why ResNet Over Alternatives?\n",
        "ResNet: Its residual connections enable training very deep networks (e.g., 50+ layers) without degradation, making it robust for complex X-ray patterns (e.g., distinguishing subtle pneumonia infiltrates from COVID-19 opacities). It's outperformed simpler models like VGGNet on image classification tasks and is more parameter-efficient than Inception variants for fine-tuning.\n",
        "Comparison to Inception (GoogLeNet): Inception is excellent for multi-scale feature extraction (via parallel branches), which could help with varying X-ray resolutions. However, ResNet often generalizes better on medical data with limited samples due to its depth and skip connections, leading to faster convergence and higher accuracy (e.g., ResNet-50 achieves ~90-95% accuracy on similar chest X-ray datasets like ChestX-ray14).\n",
        "Avoiding From-Scratch Training: Training a full CNN like AlexNet or LeNet from scratch would require massive data and risk overfitting/underfitting on small datasets.\n",
        "Performance Expectations: With transfer learning, expect 85-95% accuracy on validation sets, depending on data quality. ResNet's design helps with class imbalance (e.g., fewer COVID-19 samples) via techniques like weighted loss.\n",
        "Computational Feasibility: Fine-tuning ResNet-50 takes hours on a GPU, far less than training from scratch, aligning with startup constraints.\n",
        "Outline of Deployment Strategy\n",
        "Data Preparation and Preprocessing:\n",
        "\n",
        "Collect and curate a balanced dataset (e.g., from public sources like NIH ChestX-ray or COVID-19 image repositories). Ensure ethical use (e.g., anonymized, consented data).\n",
        "Augment data: Apply rotations, flips, brightness adjustments, and noise to artificially increase samples (e.g., using Albumentations library).\n",
        "Preprocess: Resize images to 224x224 (ResNet input), normalize pixel values, and split into train/validation/test sets (e.g., 70/20/10).\n",
        "Model Development and Training:\n",
        "\n",
        "Use PyTorch or TensorFlow: Load pre-trained ResNet-50, replace the final fully connected layer with 3 outputs (normal, pneumonia, COVID-19), and freeze early layers.\n",
        "Train with cross-entropy loss, Adam optimizer (lr=1e-4), and early stopping to prevent overfitting. Use class weights for imbalance.\n",
        "Evaluate with metrics: Accuracy, F1-score, precision/recall (prioritize recall for COVID-19 to minimize false negatives). Aim for >90% on key metrics.\n",
        "Production Deployment:\n",
        "\n",
        "API Development: Wrap the model in a REST API using Flask or FastAPI. Endpoint accepts image uploads, preprocesses them, runs inference, and returns classification probabilities with confidence scores.\n",
        "Containerization and Scaling: Use Docker to package the model, dependencies, and API. Deploy on cloud platforms like AWS SageMaker, Google AI Platform, or Azure ML for auto-scaling. Integrate with Kubernetes for high availability.\n",
        "Monitoring and Maintenance: Implement logging (e.g., via ELK stack) to track predictions. Use tools like MLflow for model versioning and retraining. Set up alerts for performance drift (e.g., if accuracy drops below 85%).\n",
        "Security and Ethics: Ensure HIPAA/GDPR compliance for medical data. Add explainability (e.g., Grad-CAM for heatmaps on X-rays) to build trust. Regularly audit for bias (e.g., across demographics) and update with new data.\n",
        "Integration: Connect to hospital systems (e.g., via DICOM) for real-time use. Estimate costs: ~$50-200/month for cloud inference on moderate traffic.\n",
        "This approach balances effectiveness, efficiency, and practicality for a healthcare AI startup, minimizing risks with limited data while enabling scalable, ethical deployment. If data grows, we could explore ensemble methods (e.g., ResNet + Inception).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jlIASiSoGgoZ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}