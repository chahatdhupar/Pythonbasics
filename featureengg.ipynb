{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1.\tWhat is a parameter?\n",
        "\n",
        "A parameter is a numerical value that summarizes data for the entire population (e.g., population mean μ, population variance σ²). In ML, parameters are weights or coefficients learned during training (e.g., in linear regression).\n",
        "\n",
        "\n",
        "2.\t What is correlation? What does negative correlation mean?\n",
        "\n",
        "Definition: Correlation measures the degree to which two variables move in relation to each other.\n",
        "Type: It can be positive, negative, or zero.\n",
        "\n",
        "Negative correlation means that as one variable increases, the other decreases.\n",
        "Example: Hours of exercise vs. body weight (more exercise, less weight).\n",
        "\n",
        "3. Define Machine Learning. What are the main components in Machine Learning?\n",
        "\n",
        "\n",
        "Definition: Machine Learning is a subset of artificial intelligence that enables systems to learn from data and improve their performance over time without being explicitly programmed\n",
        "\n",
        "Main components:\n",
        "\n",
        "Data (input)\n",
        "\n",
        "Model (algorithm)\n",
        "\n",
        "Loss function (measures error)\n",
        "\n",
        "Optimizer (improves model parameters)\n",
        "\n",
        "Evaluation metrics (accuracy, RMSE, etc.)\n",
        "\n",
        "\n",
        "4. How does loss value help in determining whether the model is good or not?\n",
        "\n",
        "The loss value indicates how far predictions are from actual outcomes.\n",
        "\n",
        "Low loss = better model performance\n",
        "\n",
        "High loss = poor predictions\n",
        "\n",
        "5.\tWhat are continuous and categorical variables?\n",
        "\n",
        "Continuous Variables: These can take an infinite number of values within a given range (e.g., height, weight).\n",
        "Categorical Variables: These represent distinct categories or groups and can be nominal (unordered) or ordinal (ordered).\n",
        "\n",
        "\n",
        "6.\t How do we handle categorical variables in Machine Learning? What are the common techniques?\n",
        "\n",
        "Label Encoding: Assigns numbers to categories.\n",
        "\n",
        "One-Hot Encoding: Creates binary columns for each category.\n",
        "\n",
        "Target Encoding: Replaces categories with target mean.\n",
        "\n",
        "7.\t What do you mean by training and testing a dataset?\n",
        "\n",
        "\n",
        "Training Dataset: Used to train the model by adjusting parameters.\n",
        "Testing Dataset: Used to evaluate the model’s performance on unseen data.\n",
        "\n",
        "\n",
        "8.\tWhat is sklearn.preprocessing?\n",
        "\n",
        "It's a module in the scikit-learn library that provides utilities for preprocessing data, such as scaling, encoding categorical variables, and normalization.\n",
        "\n",
        "\n",
        "9.\t What is a Test set?\n",
        "\n",
        "\n",
        "A test set is a portion of the dataset that is set aside to evaluate the performance of a model after training.\n",
        "\n",
        "\n",
        "10.\t How do we split data for model fitting (training and testing) in Python? How do you approach a Machine Learning problem?\n",
        "\n",
        "Data Splitting: Typically done using train_test_split() from scikit-learn.\n",
        "Approach:\n",
        "Define the Problem\n",
        "Collect Data\n",
        "Preprocess Data\n",
        "Choose a Model\n",
        "Train and Evaluate the Model\n",
        "\n",
        "11.\tWhy do we have to perform EDA before fitting a model to the data?\n",
        "\n",
        "Purpose: Exploratory Data Analysis (EDA) allows us to understand the data, detect patterns, identify anomalies, and spot outliers, which is crucial for effective modeling.\n",
        "\n",
        "\n",
        "12.\t What is correlation?\n",
        "\n",
        "Definition: Correlation measures the degree to which two variables move in relation to each other.\n",
        "Type: It can be positive, negative, or zero.\n",
        "\n",
        "13.\t What does negative correlation mean?\n",
        "\n",
        "Negative correlation means that as one variable increases, the other decreases.\n",
        "Example: Hours of exercise vs. body weight (more exercise, less weight).\n",
        "\n",
        "14.\t How can you find correlation between variables in Python?\n",
        "\n",
        "\n",
        "Method: Use the .corr() method on a DataFrame from pandas.\n",
        "\n",
        "\n",
        "15.\t What is causation? Explain difference between correlation and causation with an example.\n",
        "\n",
        "\n",
        "Causation: Indicates that one event is the result of the occurrence of another event.\n",
        "Example: Correlation between ice cream sales and drowning incidents doesn't imply causation; both increase in summer months.\n",
        "\n",
        "\n",
        "16.\tWhat is an Optimizer? What are different types of optimizers? Explain each with an example.\n",
        "\n",
        "\n",
        "Definition: An optimizer adjusts the parameters of the model to minimize the loss function.\n",
        "Types:\n",
        "Gradient Descent: Iteratively updates parameters using the gradient of the loss function.\n",
        "Adam: Combines the advantages of two other extensions of stochastic gradient descent.\n",
        "RMSprop: Adapts the learning rate based on the average of recent gradients.\n",
        "\n",
        "\n",
        "17.\tWhat is sklearn.linear_model ?\n",
        "\n",
        " A module in scikit-learn that provides linear models, such as Linear Regression and Logistic Regression.\n",
        "\n",
        "\n",
        "18.\t What does model.fit() do? What arguments must be given?\n",
        "\n",
        "Functionality: It trains the model on the provided dataset.\n",
        "Arguments: Typically requires features (X) and target (y).\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "19.\tWhat does model.predict() do? What arguments must be given?\n",
        "\n",
        "Functionality: It generates predictions based on the trained model.\n",
        "Arguments: Requires input features (X).\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "\n",
        "20.\tWhat are continuous and categorical variables?\n",
        "\n",
        "\n",
        "Continuous Variables: These can take an infinite number of values within a given range (e.g., height, weight).\n",
        "Categorical Variables: These represent distinct categories or groups and can be nominal (unordered) or ordinal (ordered).\n",
        "\n",
        "21.\tWhat is feature scaling? How does it help in Machine Learning?\n",
        "\n",
        "Definition: The process of normalizing or standardizing the range of independent variables or features.\n",
        "Benefit: Improves convergence speed and model performance.\n",
        "\n",
        "\n",
        "22.\t How do we perform scaling in Python?\n",
        "\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "\n",
        "\n",
        "23.\tWhat is sklearn.preprocessing?\n",
        "\n",
        "\n",
        "It's a module in the scikit-learn library that provides utilities for preprocessing data, such as scaling, encoding categorical variables, and normalization.\n",
        "\n",
        "24.\tHow do we split data for model fitting (training and testing) in Python?\n",
        "\n",
        "To split data for model fitting in Python, we typically divide the dataset into training and testing subsets. This can be accomplished using libraries like scikit-learn. Here’s a structured guideline to achieve this:\n",
        "\n",
        "\n",
        "\n",
        "25. Explain data encoding?\n",
        "\n",
        "Data encoding is a crucial step in preparing raw data for machine learning algorithms. It transforms categorical and textual data into a numerical format, which is essential because most machine learning algorithms work better with numerical inputs.\n",
        "\n",
        "Types of Data Encoding\n",
        "\n",
        "Label Encoding\n",
        "\n",
        "Converts categorical values into numeric labels.\n",
        "Each unique category is assigned an integer value.\n",
        "Example: {\"red\": 0, \"green\": 1, \"blue\": 2}.\n",
        "\n",
        "\n",
        "One-Hot Encoding\n",
        "\n",
        "Converts categorical variables into binary vectors.\n",
        "Each category is represented as a binary vector where one element is '1' (indicating the presence) and others are '0'.\n",
        "Example: For colors “red”, “green”, “blue”, the one-hot encoded vectors would be:\n",
        "red: [1, 0, 0]\n",
        "green: [0, 1, 0]\n",
        "blue: [0, 0, 1]\n",
        "\n",
        "\n",
        "Binary Encoding\n",
        "\n",
        "Combines features of both label encoding and one-hot encoding.\n",
        "First, it converts categories into numerical labels and then converts those numbers into binary code.\n",
        "It reduces dimensionality compared to one-hot encoding for high cardinality categories.\n",
        "\n",
        "\n",
        "Target Encoding\n",
        "\n",
        "Replaces categories with a statistic (like the mean) based on the target variable.\n",
        "Useful for high cardinality categorical variables.\n"
      ],
      "metadata": {
        "id": "xzm8S-szsBXK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jbBbTqhRr9HP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Example features and target variable\n",
        "X = np.array([[1], [2], [3], [4], [5], [6]])\n",
        "y = np.array([0, 1, 0, 1, 0, 1])\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Output the shapes\n",
        "print(\"X_train shape:\", X_train.shape)\n",
        "print(\"X_test shape:\", X_test.shape)\n",
        "print(\"y_train shape:\", y_train.shape)\n",
        "print(\"y_test shape:\", y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LpcXaoHyxKip",
        "outputId": "498bbf27-881f-4013-d1b2-875daf702163"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train shape: (4, 1)\n",
            "X_test shape: (2, 1)\n",
            "y_train shape: (4,)\n",
            "y_test shape: (2,)\n"
          ]
        }
      ]
    }
  ]
}