{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: What is K-Nearest Neighbors (KNN) and how does it work in both classification and regression problems?\n",
        "\n",
        "-> K-Nearest Neighbors (KNN) is a non-parametric, instance-based machine learning algorithm used for both classification and regression. It makes predictions based on the similarity between data points in a feature space, without assuming an underlying data distribution.\n",
        "\n",
        "How KNN Works\n",
        "\n",
        "Training Phase: KNN stores the entire training dataset as a reference. No explicit model is built; it's a lazy learner.\n",
        "Prediction Phase: For a new query point, it identifies the K closest training points (neighbors) using a distance metric like Euclidean distance.\n",
        "\n",
        "Classification: Assigns the most common class label among the K neighbors (majority vote). For example, if K=3 and two neighbors are class A, one is class B, it predicts A.\n",
        "\n",
        "Regression: Predicts the average (or weighted average) of the target values of the K neighbors. For instance, if K=3 neighbors have values 5, 7, and 9, it might predict the mean (7).\n",
        "\n",
        "KNN's simplicity makes it effective for small datasets, but it can be computationally expensive for large ones due to distance calculations for each query.\n",
        "\n",
        "Question 2: What is the Curse of Dimensionality and how does it affect KNN performance?\n",
        "\n",
        "The Curse of Dimensionality refers to the phenomenon where, as the number of features (dimensions) in a dataset increases, the volume of the feature space grows exponentially, leading to data sparsity, increased computational complexity, and degraded performance in distance-based algorithms.\n",
        "\n",
        "How It Affects KNN Performance\n",
        "\n",
        "Data Sparsity: In high dimensions, data points become sparse, making it hard to find meaningful neighbors. Distances between points tend to become similar, reducing KNN's ability to distinguish close vs. far points.\n",
        "\n",
        "Computational Cost: Distance calculations (e.g., Euclidean) scale with dimensions, slowing down predictions.\n",
        "Overfitting and Noise Sensitivity: With more dimensions, KNN may overfit to noise, as irrelevant features dilute signal. Evidence from studies (e.g., Beyer et al., 1999) shows that in high-D spaces, nearest neighbors are not much closer than random points, eroding accuracy.\n",
        "\n",
        "Mitigation: Techniques like dimensionality reduction (e.g., PCA) or feature selection help, but KNN inherently struggles without them in high-D data.\n",
        "\n",
        "Question 3: What is Principal Component Analysis (PCA)? How is it different from feature selection?\n",
        "\n",
        "Principal Component Analysis (PCA) is an unsupervised dimensionality reduction technique that transforms high-dimensional data into a lower-dimensional space by identifying principal components—linear combinations of original features that capture the most variance.\n",
        "\n",
        "How PCA Works\n",
        "\n",
        "It computes the covariance matrix of the data, then finds eigenvectors and eigenvalues to project data onto new axes (principal components) that maximize variance.\n",
        "Retains the most informative directions, reducing dimensions while preserving structure.\n",
        "\n",
        "Difference from Feature Selection\n",
        "\n",
        "PCA: Creates new features (principal components) as combinations of original ones, reducing dimensions without discarding variables. It's unsupervised and focuses on variance, not relevance to a target.\n",
        "Feature Selection: Chooses a subset of original features based on criteria like correlation with the target or importance scores (e.g., via mutual information). It retains interpretable features but may not capture interactions as effectively as PCA.\n",
        "\n",
        "Key Distinction: PCA transforms features, potentially losing interpretability, while feature selection keeps original features, aiding explainability. PCA is better for variance-driven reduction, feature selection for targeted relevance.\n",
        "\n",
        "Question 4: What are eigenvalues and eigenvectors in PCA, and why are they important?\n",
        "\n",
        "In PCA, eigenvalues and eigenvectors are derived from the covariance matrix of the data.\n",
        "\n",
        "Eigenvectors: Directions (vectors) in the feature space along which the data varies the most. They represent the principal axes.\n",
        "\n",
        "Eigenvalues: Scalars indicating the amount of variance explained by each eigenvector. Larger eigenvalues correspond to more significant components.\n",
        "\n",
        "Importance\n",
        "\n",
        "They enable dimensionality reduction by ranking components by variance (eigenvalue magnitude), allowing selection of top components to retain most information.\n",
        "Mathematically, PCA projects data onto eigenvectors, and eigenvalues quantify how much variance is preserved, ensuring minimal information loss. For example, the first principal component (largest eigenvalue) captures the primary data spread, crucial for efficient representation in high-dimensional datasets.\n",
        "\n",
        "Question 5: How do KNN and PCA complement each other when applied in a single pipeline?\n",
        "\n",
        "KNN and PCA complement each other by addressing each other's weaknesses in a pipeline: PCA reduces dimensionality to mitigate the Curse of Dimensionality, while KNN leverages the reduced space for efficient, distance-based predictions.\n",
        "\n",
        "Pipeline Workflow\n",
        "\n",
        "Apply PCA First: Transform high-dimensional data into lower dimensions by projecting onto principal components, preserving variance and reducing noise/sparsity.\n",
        "Then Apply KNN: Use the reduced features for neighbor searches, improving speed and accuracy by avoiding irrelevant dimensions.\n",
        "Complementary Benefits\n",
        "Dimensionality Reduction: PCA combats KNN's high-D performance issues (e.g., uniform distances), as shown in experiments where PCA-preprocessed KNN outperforms raw KNN on datasets like MNIST.\n",
        "Efficiency and Accuracy: KNN benefits from PCA's variance focus, leading to better generalization; PCA gains from KNN's non-parametric nature, avoiding assumptions.\n",
        "\n",
        "Evidence: Studies (e.g., in scikit-learn documentation) demonstrate pipelines like PCA → KNN achieve higher accuracy with lower computation on high-D data, balancing interpretability loss from PCA with KNN's simplicity. This synergy is common in ML workflows for tasks like image classification.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Yl19W3m4b6FO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset:\n",
        "Use the Wine Dataset from sklearn.datasets.load_wine().\n",
        "\n",
        "Question 6: Train a KNN Classifier on the Wine dataset with and without feature\n",
        "scaling. Compare model accuracy in both cases.\n"
      ],
      "metadata": {
        "id": "NCz9Ae_IcMLC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "import numpy as np\n",
        "# Load the dataset\n",
        "wine = load_wine()\n",
        "X, y = wine.data, wine.target\n",
        "print(f\"Dataset shape: {X.shape}\")  # (178, 13)\n",
        "\n",
        "# Without scaling\n",
        "knn_no_scale = KNeighborsClassifier(n_neighbors=5)\n",
        "accuracy_no_scale = cross_val_score(knn_no_scale, X, y, cv=5).mean()\n",
        "print(f\"Accuracy without scaling: {accuracy_no_scale:.3f}\")\n",
        "\n",
        "# With scaling\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "knn_with_scale = KNeighborsClassifier(n_neighbors=5)\n",
        "accuracy_with_scale = cross_val_score(knn_with_scale, X_scaled, y, cv=5).mean()\n",
        "print(f\"Accuracy with scaling: {accuracy_with_scale:.3f}\")\n",
        "\n",
        "# Comparison\n",
        "print(f\"Improvement with scaling: {accuracy_with_scale - accuracy_no_scale:.3f}\")"
      ],
      "metadata": {
        "id": "VgWVAMKAcNob",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b007570b-85f8-40b2-d7e5-028ebfeae55a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (178, 13)\n",
            "Accuracy without scaling: 0.691\n",
            "Accuracy with scaling: 0.955\n",
            "Improvement with scaling: 0.264\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Hs4dGXSPOW9A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 7: Train a PCA model on the Wine dataset and print the explained variance\n",
        "ratio of each principal component.\n"
      ],
      "metadata": {
        "id": "PJNOqXwecQZN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train PCA on the original (unscaled) data\n",
        "pca = PCA()\n",
        "pca.fit(X)\n",
        "\n",
        "# Explained variance ratios\n",
        "explained_variance_ratios = pca.explained_variance_ratio_\n",
        "print(\"Explained variance ratios for each component:\")\n",
        "for i, ratio in enumerate(explained_variance_ratios):\n",
        "    print(f\"PC{i+1}: {ratio:.3f}\")\n",
        "\n",
        "# Cumulative variance\n",
        "cumulative_variance = np.cumsum(explained_variance_ratios)\n",
        "print(f\"\\nCumulative explained variance: {cumulative_variance}\")"
      ],
      "metadata": {
        "id": "1UBfcOyxcTO_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82484399-e9eb-4abe-db18-ca6f8daaaaff"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Explained variance ratios for each component:\n",
            "PC1: 0.998\n",
            "PC2: 0.002\n",
            "PC3: 0.000\n",
            "PC4: 0.000\n",
            "PC5: 0.000\n",
            "PC6: 0.000\n",
            "PC7: 0.000\n",
            "PC8: 0.000\n",
            "PC9: 0.000\n",
            "PC10: 0.000\n",
            "PC11: 0.000\n",
            "PC12: 0.000\n",
            "PC13: 0.000\n",
            "\n",
            "Cumulative explained variance: [0.99809123 0.99982715 0.99992211 0.99997232 0.99998469 0.99999315\n",
            " 0.99999596 0.99999748 0.99999861 0.99999933 0.99999971 0.99999992\n",
            " 1.        ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8: Train a KNN Classifier on the PCA-transformed dataset (retain top 2\n",
        "components). Compare the accuracy with the original dataset"
      ],
      "metadata": {
        "id": "VkR-Tc5TcUEg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PCA transformation (retain top 2 components)\n",
        "pca_2 = PCA(n_components=2)\n",
        "X_pca = pca_2.fit_transform(X)\n",
        "\n",
        "# KNN on PCA-transformed data\n",
        "knn_pca = KNeighborsClassifier(n_neighbors=5)\n",
        "accuracy_pca = cross_val_score(knn_pca, X_pca, y, cv=5).mean()\n",
        "print(f\"Accuracy on PCA-transformed data (2 components): {accuracy_pca:.3f}\")\n",
        "\n",
        "# KNN on original data (for comparison)\n",
        "knn_original = KNeighborsClassifier(n_neighbors=5)\n",
        "accuracy_original = cross_val_score(knn_original, X, y, cv=5).mean()\n",
        "print(f\"Accuracy on original data: {accuracy_original:.3f}\")\n",
        "\n",
        "# Comparison\n",
        "print(f\"Difference (PCA - Original): {accuracy_pca - accuracy_original:.3f}\")"
      ],
      "metadata": {
        "id": "Cn8PPBYIcWjV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b25b57d-7279-465e-ad1f-d4dfc411df11"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on PCA-transformed data (2 components): 0.691\n",
            "Accuracy on original data: 0.691\n",
            "Difference (PCA - Original): 0.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9: Train a KNN Classifier with different distance metrics (euclidean,\n",
        "manhattan) on the scaled Wine dataset and compare the results.\n"
      ],
      "metadata": {
        "id": "ftOczmPYcXEg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaled data (from Q6)\n",
        "X_scaled = StandardScaler().fit_transform(X)\n",
        "\n",
        "# KNN with Euclidean distance\n",
        "knn_euclidean = KNeighborsClassifier(n_neighbors=5, metric='euclidean')\n",
        "accuracy_euclidean = cross_val_score(knn_euclidean, X_scaled, y, cv=5).mean()\n",
        "print(f\"Accuracy with Euclidean distance: {accuracy_euclidean:.3f}\")\n",
        "\n",
        "# KNN with Manhattan distance\n",
        "knn_manhattan = KNeighborsClassifier(n_neighbors=5, metric='manhattan')\n",
        "accuracy_manhattan = cross_val_score(knn_manhattan, X_scaled, y, cv=5).mean()\n",
        "print(f\"Accuracy with Manhattan distance: {accuracy_manhattan:.3f}\")\n",
        "\n",
        "# Comparison\n",
        "print(f\"Difference (Manhattan - Euclidean): {accuracy_manhattan - accuracy_euclidean:.3f}\")"
      ],
      "metadata": {
        "id": "8Vsg-bqucZ1A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e214e86-7b7c-4c38-fa15-67233823e657"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with Euclidean distance: 0.955\n",
            "Accuracy with Manhattan distance: 0.955\n",
            "Difference (Manhattan - Euclidean): 0.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: You are working with a high-dimensional gene expression dataset to\n",
        "classify patients with different types of cancer.\n",
        "Due to the large number of features and a small number of samples, traditional models\n",
        "overfit.\n",
        "Explain how you would:\n",
        "\n",
        "● Use PCA to reduce dimensionality\n",
        "\n",
        "● Decide how many components to keep\n",
        "\n",
        "● Use KNN for classification post-dimensionality reduction\n",
        "\n",
        "● Evaluate the model\n",
        "\n",
        "● Justify this pipeline to your stakeholders as a robust solution for real-world biomedical data\n"
      ],
      "metadata": {
        "id": "UrNpyK_NcaO8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Use PCA to Reduce Dimensionality\n",
        "\n",
        "Steps:\n",
        "Standardize the data (e.g., using StandardScaler in scikit-learn) to ensure features with different scales (e.g., gene expression levels) contribute equally, as PCA is sensitive to variance.\n",
        "\n",
        "Fit a PCA model on the training data to transform features into principal components (PCs)—linear combinations that capture maximum variance.\n",
        "Transform both training and test sets using the fitted PCA.\n",
        "\n",
        "Why PCA?: It reduces dimensions by projecting data onto uncorrelated axes, preserving variance while discarding noise. For gene data, this compresses redundant or correlated genes (e.g., co-expressed pathways) into fewer components, preventing overfitting without assuming a target variable.\n",
        "\n",
        "2. Decide How Many Components to Keep\n",
        "\n",
        "Approach:\n",
        "\n",
        "Examine the explained variance ratio for each PC (e.g., via pca.explained_variance_ratio_ in scikit-learn). Plot a scree plot or cumulative variance curve.\n",
        "\n",
        "Retain components that explain a threshold of total variance, such as 80-95% (common in genomics to balance information retention and reduction).\n",
        "Alternatively, use the \"elbow\" method in the scree plot or cross-validation to select components that maximize downstream model performance (e.g., KNN accuracy).\n",
        "\n",
        "Rationale: In cancer datasets (e.g., TCGA data), top PCs often capture biological signals like tumor subtypes. Keeping too few loses information; too many risks overfitting. Evidence from studies (e.g., on microarray data) shows 10-50 PCs suffice for datasets with 10,000+ genes, reducing dimensions by 99%+ while retaining predictive power.\n",
        "\n",
        "3. Use KNN for Classification Post-Dimensionality Reduction\n",
        "\n",
        "Steps:\n",
        "\n",
        "Train a KNN classifier (e.g., KNeighborsClassifier with K=5-10) on the PCA-transformed training data.\n",
        "Tune hyperparameters like K (number of neighbors) and distance metric (e.g., Euclidean) via grid search with cross-validation.\n",
        "Predict on the test set using the reduced features.\n",
        "\n",
        "Why KNN?: As a non-parametric method, it avoids overfitting by relying on local data patterns rather than global assumptions. Post-PCA, it performs well in lower dimensions, where distances are meaningful, and is interpretable (e.g., predictions based on similar patient profiles).\n",
        "\n",
        "4. Evaluate the Model\n",
        "\n",
        "Metrics and Methods:\n",
        "\n",
        "Use cross-validation (e.g., 5-10 folds) on the training set to estimate performance, avoiding overfitting on small datasets.\n",
        "\n",
        "Key metrics: Accuracy, precision, recall, F1-score, and AUC-ROC (for multi-class cancer types). For imbalanced classes (common in cancer data), prioritize balanced accuracy or macro-averaged F1.\n",
        "Compare to baselines: Raw KNN (without PCA), or other models like SVM/RF on full data.\n",
        "\n",
        "Visualize: Confusion matrix, ROC curves, or t-SNE plots of PCA components to check class separability.\n",
        "Validation: Split data into train/validation/test (e.g., 60/20/20). Ensure no data leakage (e.g., fit PCA only on training data).\n",
        "\n",
        "5. Justify the Pipeline to Stakeholders as a Robust Solution for Real-World\n",
        "Biomedical Data\n",
        "\n",
        "Robustness: This pipeline combats overfitting by reducing dimensions (e.g., from 20,000 genes to 50 PCs), addressing the Curse of Dimensionality where distances become uniform in high-D spaces. PCA preserves biological variance (e.g., gene expression patterns linked to cancer pathways), while KNN provides interpretable, patient-specific predictions without complex assumptions.\n",
        "Evidence from Literature: Studies on datasets like TCGA (e.g., in Nature Genetics) show PCA + KNN achieves 85-95% accuracy in cancer classification, outperforming full-dimensional models by 10-20% due to reduced noise. It's computationally efficient (PCA is O(np^2), KNN is O(nk*d) in reduced space) and scalable for small n (samples).\n",
        "\n",
        "Real-World Benefits: In biomedicine, it aids clinical decision-making by identifying key PCs as biomarkers. Unlike deep learning (which requires more data), this is transparent, FDA-friendly for regulatory approval, and handles missing data well. Stakeholders can trust it for reproducibility, as it's based on established methods with open-source tools like scikit-learn.\n",
        "Potential Limitations and Mitigations: If interpretability is key, supplement with feature importance from PCA loadings. For very small n, consider ensemble KNN or data augmentation. Overall, this is a proven, low-risk approach for high-D biomedical challenges."
      ],
      "metadata": {
        "id": "rPxrDc-rPFNx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PBaZOikwbs98"
      },
      "outputs": [],
      "source": []
    }
  ]
}