{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: What is a Decision Tree, and how does it work in the context of classification?\n",
        "\n",
        "\n",
        "A Decision Tree is a supervised machine learning algorithm used for both classification and regression tasks, but it is particularly intuitive for classification. It models decisions in the form of a tree-like structure, where the goal is to predict the class label of input data by traversing from the root to a leaf node based on feature values.\n",
        "\n",
        "In the context of classification:\n",
        "\n",
        "The tree starts with a root node representing the entire dataset.\n",
        "Internal nodes represent decision points based on a selected feature (e.g., \"Is age > 30?\").\n",
        "Branches represent the outcome of the decision (e.g., yes or no), leading to child nodes.\n",
        "Leaf nodes represent the final predicted class (e.g., \"Class A\" or \"Class B\").\n",
        "The algorithm works by recursively partitioning the dataset:\n",
        "\n",
        "At each node, it evaluates all possible splits on features to find the one that best separates the data into purer subsets (i.e., subsets where most samples belong to the same class).\n",
        "The \"best\" split is chosen using an impurity measure (e.g., Gini Impurity or Entropy;).\n",
        "This process repeats for each child node until a stopping criterion is met, such as:\n",
        "All samples in a node belong to the same class.\n",
        "A maximum tree depth is reached.\n",
        "No further splits improve purity significantly.\n",
        "For prediction, new data follows the path from root to leaf based on its feature values, and the leaf's majority class is assigned.\n",
        "This top-down, greedy approach makes Decision Trees easy to visualize and interpret, mimicking human decision-making\n",
        "\n",
        "\n",
        "Question 2: Explain the concepts of Gini Impurity and Entropy as impurity measures.How do they impact the splits in a Decision Tree?\n",
        "\n",
        "Gini Impurity and Entropy are both metrics used to quantify the \"impurity\" or heterogeneity of a node's samples in a Decision Tree. They help evaluate how well a split divides the data into purer subsets (where samples are more likely to belong to the same class). The split that maximizes the reduction in impurity is selected.\n",
        "\n",
        "Gini Impurity: This measures the probability of incorrectly classifying a randomly selected sample from the node if it were labeled according to the distribution of classes in that node. It favors splits that create nodes with samples predominantly from one class. The formula for a node with\n",
        " classes and class probabilities\n",
        " (where\n",
        ") is:\n",
        "      Gini=1−∑pi2​\n",
        "\n",
        "Range: 0 (perfectly pure, all samples same class) to 0.5 (maximum impurity for binary classification).\n",
        "\n",
        "Entropy: Borrowed from information theory, this measures the average uncertainty or information required to predict a class. It penalizes mixed classes more logarithmically. The formula is:\n",
        "            Entropy=−∑pi​log2​(pi​)\n",
        "\n",
        "Range: 0 (pure node) to 1 (maximum for binary classification with equal probabilities).\n",
        "\n",
        "Impact on splits:\n",
        "\n",
        "To choose a split, the algorithm computes the impurity reduction (or gain) for each possible feature split:\n",
        "For Gini: Weighted average impurity of child nodes subtracted from parent impurity.\n",
        "For Entropy: This leads to Information Gain.\n",
        "Splits that result in the largest reduction (i.e., lowest weighted child impurity) are preferred, as they create more homogeneous subsets. Gini is computationally faster (no logs), while Entropy may lead to slightly different trees but similar performance. Both encourage balanced, informative splits, reducing overfitting by stopping when gains are minimal.\n",
        "\n",
        "\n",
        "Question 3: What is the difference between Pre-Pruning and Post-Pruning in Decision Trees? Give one practical advantage of using each.\n",
        "\n",
        "Pruning is a technique to prevent overfitting in Decision Trees by limiting tree complexity. The key difference lies in when the pruning occurs relative to tree construction.\n",
        "\n",
        "Pre-Pruning (also called early stopping): This involves halting tree growth before the full tree is built, using predefined criteria during the splitting process. Examples include:\n",
        "\n",
        "Maximum tree depth (e.g., stop at depth 5).\n",
        "Minimum samples per leaf (e.g., at least 10 samples).\n",
        "Minimum impurity reduction threshold (e.g., only split if gain > 0.01).\n",
        "It checks these rules at each potential split and stops if violated.\n",
        "Post-Pruning (also called subtree raising or cost-complexity pruning): This builds the full, unpruned tree first (allowing overfitting on training data), then prunes it afterward by removing branches that do not improve performance on a validation set. It uses metrics like error rate or cross-validation to decide which subtrees to collapse into leaves.\n",
        "\n",
        "Practical advantages:\n",
        "\n",
        "Pre-Pruning advantage: It is computationally efficient, as it avoids building and evaluating an overly large tree, making training faster—especially useful for large datasets or real-time applications.\n",
        "Post-Pruning advantage: It can produce more accurate models by first exploring the full structure, allowing for better generalization; this is beneficial when the optimal tree size is unknown upfront, as it relies on empirical validation rather than heuristic thresholds.\n",
        "\n",
        "\n",
        "Question 4: What is Information Gain in Decision Trees, and why is it important for choosing the best split?\n",
        "\n",
        "\n",
        "Information Gain (IG) is a metric used in Decision Trees (particularly with Entropy as the impurity measure) to quantify the effectiveness of a potential split on a feature. It represents the reduction in uncertainty (entropy) about the class labels after splitting the data.\n",
        "\n",
        "IG=Entropy (before split)−Entropy (after split)\n",
        "\n",
        "Importance for choosing the best split:\n",
        "\n",
        "At each node, the algorithm calculates IG for every possible feature and value (or threshold for continuous features). The feature with the highest IG is selected, as it provides the most information about separating classes—maximizing purity in children.\n",
        "\n",
        "This greedy selection ensures the tree prioritizes informative features early, leading to efficient, hierarchical decisions. Without IG (or similar metrics like Gini Gain), splits would be arbitrary, resulting in poor predictive performance. It's crucial for handling high-dimensional data by ranking feature relevance.\n",
        "\n",
        "\n",
        "Question 5: What are some common real-world applications of Decision Trees, and what are their main advantages and limitations?\n",
        "\n",
        "\n",
        "Common real-world applications:\n",
        "\n",
        "Medical Diagnosis: Classifying diseases (e.g., predicting cancer risk from symptoms and test results in tools like IBM Watson Health).\n",
        "\n",
        "Credit Risk Assessment: Banks use them to approve loans by classifying applicants as low/high risk based on income, credit history, etc.\n",
        "\n",
        "Customer Segmentation: E-commerce platforms (e.g., Amazon) segment users for targeted marketing, classifying behaviors like \"likely to churn.\"\n",
        "\n",
        "Fraud Detection: In finance or e-commerce, detecting anomalous transactions (e.g., PayPal classifying as fraudulent or legitimate).\n",
        "\n",
        "Environmental Modeling: Predicting land cover types from satellite imagery for agriculture or conservation.\n",
        "\n",
        "\n",
        "Main advantages:\n",
        "\n",
        "Interpretability: The tree structure is easy to visualize and explain (e.g., \"If age > 50 and income < 40k, then high risk\"), making it suitable for domains requiring transparency like healthcare or finance.\n",
        "\n",
        "Handles Mixed Data: Works with categorical and numerical features without needing scaling or encoding (beyond one-hot for categories).\n",
        "\n",
        "Captures Non-Linear Relationships: Naturally models interactions between features without assuming linearity, unlike logistic regression.\n",
        "\n",
        "No Assumptions: Requires minimal data preprocessing and handles missing values via surrogate splits.\n",
        "\n",
        "\n",
        "Main limitations:\n",
        "\n",
        "Overfitting: Deep trees can memorize training data, performing poorly on unseen data; pruning or ensemble methods (e.g., Random Forests) are often needed to mitigate this.\n",
        "\n",
        "Instability: Small changes in data can lead to very different tree structures, making predictions sensitive to noise.\n",
        "\n",
        "Bias Toward Dominant Classes/Features: Prefers features with more levels (e.g., categorical with many categories) and can struggle with imbalanced datasets unless weighted.\n",
        "\n",
        "Scalability Issues: While efficient for moderate data, exhaustive splits on high-cardinality features can be computationally expensive; not ideal for very large datasets without optimizations.\n",
        "\n",
        "Overall, Decision Trees are a foundational algorithm, often serving as building blocks for more robust ensembles like Random Forests or Gradient Boosting Machines.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zLTqpyTCc3kc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 6: Write a Python program to:\n",
        "\n",
        "● Load the Iris Dataset\n",
        "\n",
        "● Train a Decision Tree Classifier using the Gini criterion\n",
        "\n",
        "● Print the model’s accuracy and feature importances"
      ],
      "metadata": {
        "id": "gkrU5ZcyenSz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ugJXMw3Wc2qn"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Load the Iris Dataset\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features: sepal length, sepal width, petal length, petal width\n",
        "y = iris.target  # Target: class labels (0: setosa, 1: versicolor, 2: virginica)"
      ],
      "metadata": {
        "id": "vwxYEgBcNkjS"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "L55j4ubMNtWy"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Train a Decision Tree Classifier using the Gini criterion\n",
        "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "clf.fit(X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "tsopgNfRN38u",
        "outputId": "fe2f79ce-2e2e-4394-a059-85745b4bda72"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DecisionTreeClassifier(random_state=42)"
            ],
            "text/html": [
              "<style>#sk-container-id-2 {\n",
              "  /* Definition of color scheme common for light and dark mode */\n",
              "  --sklearn-color-text: #000;\n",
              "  --sklearn-color-text-muted: #666;\n",
              "  --sklearn-color-line: gray;\n",
              "  /* Definition of color scheme for unfitted estimators */\n",
              "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
              "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
              "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
              "  --sklearn-color-unfitted-level-3: chocolate;\n",
              "  /* Definition of color scheme for fitted estimators */\n",
              "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
              "  --sklearn-color-fitted-level-1: #d4ebff;\n",
              "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
              "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
              "\n",
              "  /* Specific color for light theme */\n",
              "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
              "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-icon: #696969;\n",
              "\n",
              "  @media (prefers-color-scheme: dark) {\n",
              "    /* Redefinition of color scheme for dark theme */\n",
              "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
              "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-icon: #878787;\n",
              "  }\n",
              "}\n",
              "\n",
              "#sk-container-id-2 {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 pre {\n",
              "  padding: 0;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 input.sk-hidden--visually {\n",
              "  border: 0;\n",
              "  clip: rect(1px 1px 1px 1px);\n",
              "  clip: rect(1px, 1px, 1px, 1px);\n",
              "  height: 1px;\n",
              "  margin: -1px;\n",
              "  overflow: hidden;\n",
              "  padding: 0;\n",
              "  position: absolute;\n",
              "  width: 1px;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-dashed-wrapped {\n",
              "  border: 1px dashed var(--sklearn-color-line);\n",
              "  margin: 0 0.4em 0.5em 0.4em;\n",
              "  box-sizing: border-box;\n",
              "  padding-bottom: 0.4em;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-container {\n",
              "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
              "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
              "     so we also need the `!important` here to be able to override the\n",
              "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
              "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
              "  display: inline-block !important;\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-text-repr-fallback {\n",
              "  display: none;\n",
              "}\n",
              "\n",
              "div.sk-parallel-item,\n",
              "div.sk-serial,\n",
              "div.sk-item {\n",
              "  /* draw centered vertical line to link estimators */\n",
              "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
              "  background-size: 2px 100%;\n",
              "  background-repeat: no-repeat;\n",
              "  background-position: center center;\n",
              "}\n",
              "\n",
              "/* Parallel-specific style estimator block */\n",
              "\n",
              "#sk-container-id-2 div.sk-parallel-item::after {\n",
              "  content: \"\";\n",
              "  width: 100%;\n",
              "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
              "  flex-grow: 1;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-parallel {\n",
              "  display: flex;\n",
              "  align-items: stretch;\n",
              "  justify-content: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-parallel-item {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-parallel-item:first-child::after {\n",
              "  align-self: flex-end;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-parallel-item:last-child::after {\n",
              "  align-self: flex-start;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-parallel-item:only-child::after {\n",
              "  width: 0;\n",
              "}\n",
              "\n",
              "/* Serial-specific style estimator block */\n",
              "\n",
              "#sk-container-id-2 div.sk-serial {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "  align-items: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  padding-right: 1em;\n",
              "  padding-left: 1em;\n",
              "}\n",
              "\n",
              "\n",
              "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
              "clickable and can be expanded/collapsed.\n",
              "- Pipeline and ColumnTransformer use this feature and define the default style\n",
              "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
              "*/\n",
              "\n",
              "/* Pipeline and ColumnTransformer style (default) */\n",
              "\n",
              "#sk-container-id-2 div.sk-toggleable {\n",
              "  /* Default theme specific background. It is overwritten whether we have a\n",
              "  specific estimator or a Pipeline/ColumnTransformer */\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "/* Toggleable label */\n",
              "#sk-container-id-2 label.sk-toggleable__label {\n",
              "  cursor: pointer;\n",
              "  display: flex;\n",
              "  width: 100%;\n",
              "  margin-bottom: 0;\n",
              "  padding: 0.5em;\n",
              "  box-sizing: border-box;\n",
              "  text-align: center;\n",
              "  align-items: start;\n",
              "  justify-content: space-between;\n",
              "  gap: 0.5em;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 label.sk-toggleable__label .caption {\n",
              "  font-size: 0.6rem;\n",
              "  font-weight: lighter;\n",
              "  color: var(--sklearn-color-text-muted);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 label.sk-toggleable__label-arrow:before {\n",
              "  /* Arrow on the left of the label */\n",
              "  content: \"▸\";\n",
              "  float: left;\n",
              "  margin-right: 0.25em;\n",
              "  color: var(--sklearn-color-icon);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "/* Toggleable content - dropdown */\n",
              "\n",
              "#sk-container-id-2 div.sk-toggleable__content {\n",
              "  max-height: 0;\n",
              "  max-width: 0;\n",
              "  overflow: hidden;\n",
              "  text-align: left;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-toggleable__content.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-toggleable__content pre {\n",
              "  margin: 0.2em;\n",
              "  border-radius: 0.25em;\n",
              "  color: var(--sklearn-color-text);\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-toggleable__content.fitted pre {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
              "  /* Expand drop-down */\n",
              "  max-height: 200px;\n",
              "  max-width: 100%;\n",
              "  overflow: auto;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
              "  content: \"▾\";\n",
              "}\n",
              "\n",
              "/* Pipeline/ColumnTransformer-specific style */\n",
              "\n",
              "#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator-specific style */\n",
              "\n",
              "/* Colorize estimator box */\n",
              "#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-label label.sk-toggleable__label,\n",
              "#sk-container-id-2 div.sk-label label {\n",
              "  /* The background is the default theme color */\n",
              "  color: var(--sklearn-color-text-on-default-background);\n",
              "}\n",
              "\n",
              "/* On hover, darken the color of the background */\n",
              "#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "/* Label box, darken color on hover, fitted */\n",
              "#sk-container-id-2 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator label */\n",
              "\n",
              "#sk-container-id-2 div.sk-label label {\n",
              "  font-family: monospace;\n",
              "  font-weight: bold;\n",
              "  display: inline-block;\n",
              "  line-height: 1.2em;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-label-container {\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              "/* Estimator-specific */\n",
              "#sk-container-id-2 div.sk-estimator {\n",
              "  font-family: monospace;\n",
              "  border: 1px dotted var(--sklearn-color-border-box);\n",
              "  border-radius: 0.25em;\n",
              "  box-sizing: border-box;\n",
              "  margin-bottom: 0.5em;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-estimator.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "/* on hover */\n",
              "#sk-container-id-2 div.sk-estimator:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-estimator.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
              "\n",
              "/* Common style for \"i\" and \"?\" */\n",
              "\n",
              ".sk-estimator-doc-link,\n",
              "a:link.sk-estimator-doc-link,\n",
              "a:visited.sk-estimator-doc-link {\n",
              "  float: right;\n",
              "  font-size: smaller;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1em;\n",
              "  height: 1em;\n",
              "  width: 1em;\n",
              "  text-decoration: none !important;\n",
              "  margin-left: 0.5em;\n",
              "  text-align: center;\n",
              "  /* unfitted */\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted,\n",
              "a:link.sk-estimator-doc-link.fitted,\n",
              "a:visited.sk-estimator-doc-link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "/* Span, style for the box shown on hovering the info icon */\n",
              ".sk-estimator-doc-link span {\n",
              "  display: none;\n",
              "  z-index: 9999;\n",
              "  position: relative;\n",
              "  font-weight: normal;\n",
              "  right: .2ex;\n",
              "  padding: .5ex;\n",
              "  margin: .5ex;\n",
              "  width: min-content;\n",
              "  min-width: 20ex;\n",
              "  max-width: 50ex;\n",
              "  color: var(--sklearn-color-text);\n",
              "  box-shadow: 2pt 2pt 4pt #999;\n",
              "  /* unfitted */\n",
              "  background: var(--sklearn-color-unfitted-level-0);\n",
              "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted span {\n",
              "  /* fitted */\n",
              "  background: var(--sklearn-color-fitted-level-0);\n",
              "  border: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link:hover span {\n",
              "  display: block;\n",
              "}\n",
              "\n",
              "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
              "\n",
              "#sk-container-id-2 a.estimator_doc_link {\n",
              "  float: right;\n",
              "  font-size: 1rem;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1rem;\n",
              "  height: 1rem;\n",
              "  width: 1rem;\n",
              "  text-decoration: none;\n",
              "  /* unfitted */\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 a.estimator_doc_link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "#sk-container-id-2 a.estimator_doc_link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 a.estimator_doc_link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>DecisionTreeClassifier</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.tree.DecisionTreeClassifier.html\">?<span>Documentation for DecisionTreeClassifier</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>DecisionTreeClassifier(random_state=42)</pre></div> </div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Make predictions on the test set\n",
        "y_pred = clf.predict(X_test)"
      ],
      "metadata": {
        "id": "ch06fTnmOMgV"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Calculate and print the model's accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.4f} ({accuracy * 100:.2f}%)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MGo_99imOWGr",
        "outputId": "cc88370b-ce7a-4eab-aa67-c43e966a1d43"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.0000 (100.00%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Print the feature importances\n",
        "# Feature names for reference\n",
        "feature_names = iris.feature_names\n",
        "importances = clf.feature_importances_\n",
        "print(\"\\nFeature Importances:\")\n",
        "for name, importance in zip(feature_names, importances):\n",
        "    print(f\"{name}: {importance:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cAnKtIWtOb9j",
        "outputId": "6f7da80b-2b9c-475c-9079-0ec896683c63"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Feature Importances:\n",
            "sepal length (cm): 0.0000\n",
            "sepal width (cm): 0.0167\n",
            "petal length (cm): 0.9061\n",
            "petal width (cm): 0.0772\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 7: Write a Python program to:\n",
        "\n",
        "● Load the Iris Dataset\n",
        "\n",
        "● Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to a fully-grown tree.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "sedSGm4qepPP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "MGdzzgmIenxO"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target"
      ],
      "metadata": {
        "id": "-bZ0OXrdPiXJ"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=42)"
      ],
      "metadata": {
        "id": "NFA3NtkZPx_n"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_tree = DecisionTreeClassifier(random_state=42)\n",
        "full_tree.fit(X_train, y_train)\n",
        "y_pred_full = full_tree.predict(X_test)\n",
        "accuracy_full = accuracy_score(y_test, y_pred_full)"
      ],
      "metadata": {
        "id": "i4pi68-ORw0Q"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "limited_tree_classfier = DecisionTreeClassifier(max_depth=3,random_state=42)\n",
        "limited_tree_classfier.fit(X_train,y_train)\n",
        "y_predict = limited_tree_classfier.predict(X_test)\n",
        "accuracy_limited = accuracy_score(y_test,y_pred)"
      ],
      "metadata": {
        "id": "OKx76lymQGEU"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Fully-Grown Tree Accuracy: {accuracy_full:.4f} ({accuracy_full * 100:.2f}%)\")\n",
        "\n",
        "print(f\"Limited Tree (max_depth=3) Accuracy: {accuracy_limited:.4f} ({accuracy_limited * 100:.2f}%)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sUO2KMQsRTG_",
        "outputId": "9bcc2551-56df-486a-a0f6-06edc684af79"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fully-Grown Tree Accuracy: 1.0000 (100.00%)\n",
            "Limited Tree (max_depth=3) Accuracy: 1.0000 (100.00%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if accuracy_full > accuracy_limited:\n",
        "    print(f\"\\nThe fully-grown tree performs better by {accuracy_full - accuracy_limited:.4f} in accuracy.\")\n",
        "elif accuracy_limited > accuracy_full:\n",
        "    print(f\"\\nThe limited tree performs better by {accuracy_limited - accuracy_full:.4f} in accuracy.\")\n",
        "else:\n",
        "    print(\"\\nBoth trees have the same accuracy.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9womApKZR4FZ",
        "outputId": "a0fe0c6a-ba7d-418e-e42c-ed11af3e7b54"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Both trees have the same accuracy.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8: Write a Python program to:\n",
        "\n",
        "● Load the Boston Housing Dataset\n",
        "\n",
        "● Train a Decision Tree Regressor\n",
        "\n",
        "● Print the Mean Squared Error (MSE) and feature importances\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MwSyIN4eeoG1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "Si9WBEV3e4Th"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "california = fetch_california_housing()\n",
        "X = california.data\n",
        "y = california.target"
      ],
      "metadata": {
        "id": "9oFDQ6WtSqeL"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "7cZkYDwUSqar"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dtr = DecisionTreeRegressor(random_state=42)\n",
        "dtr.fit(X_train, y_train)\n",
        "y_pred = dtr.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)"
      ],
      "metadata": {
        "id": "1SFPqTfBSqYC"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GgALWAAVSqVd",
        "outputId": "2c0d5767-b242-4e2a-df83-0b1588fefa04"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error (MSE): 0.4952\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "feature_names = california.feature_names\n",
        "importances = dtr.feature_importances_\n",
        "print(\"\\nFeature Importances:\")\n",
        "for name, importance in zip(feature_names, importances):\n",
        "    print(f\"{name}: {importance:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yThBPLBcSqS1",
        "outputId": "685e13d3-f5ca-4665-de20-76711dd9d5ee"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Feature Importances:\n",
            "MedInc: 0.5285\n",
            "HouseAge: 0.0519\n",
            "AveRooms: 0.0530\n",
            "AveBedrms: 0.0287\n",
            "Population: 0.0305\n",
            "AveOccup: 0.1308\n",
            "Latitude: 0.0937\n",
            "Longitude: 0.0829\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9: Write a Python program to:\n",
        "\n",
        "● Load the Iris Dataset\n",
        "\n",
        "● Tune the Decision Tree’s max_depth and min_samples_split using\n",
        "GridSearchCV\n",
        "\n",
        "● Print the best parameters and the resulting model accuracy\n",
        "\n"
      ],
      "metadata": {
        "id": "pdxt71Kne41o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# 2. Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# 3. Initialize the Decision Tree model\n",
        "dtree = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# 4. Define the parameter grid for tuning\n",
        "param_grid = {\n",
        "    'max_depth': [2, 3, 4, 5, 6, None],\n",
        "    'min_samples_split': [2, 5, 10]\n",
        "}\n",
        "\n",
        "# 5. Use GridSearchCV for parameter tuning\n",
        "grid_search = GridSearchCV(estimator=dtree, param_grid=param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# 6. Print the best parameters\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "\n",
        "# 7. Evaluate the best model on the test data\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Model Accuracy:\", accuracy)"
      ],
      "metadata": {
        "id": "UrZm7MZTe6_p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87890871-085a-466c-c0b7-fe451f9b9094"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': 4, 'min_samples_split': 10}\n",
            "Model Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: Imagine you’re working as a data scientist for a healthcare company that\n",
        "wants to predict whether a patient has a certain disease. You have a large dataset with\n",
        "mixed data types and some missing values.\n",
        "Explain the step-by-step process you would follow to:\n",
        "\n",
        "● Handle the missing values\n",
        "\n",
        "● Encode the categorical features\n",
        "\n",
        "● Train a Decision Tree model\n",
        "\n",
        "● Tune its hyperparameters\n",
        "\n",
        "● Evaluate its performance\n",
        "And describe what business value this model could provide in the real-world\n",
        "setting."
      ],
      "metadata": {
        "id": "WjSLnUz-e7nQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As a data scientist in a healthcare company, I'd approach this project methodically, prioritizing data quality, model interpretability (since Decision Trees are transparent), and ethical considerations (e.g., avoiding bias in sensitive health data). The goal is binary classification: predicting disease presence (yes/no). I'd use Python with libraries like pandas, scikit-learn, and possibly imbalanced-learn for handling class imbalance common in medical datasets. Below is the detailed process.\n",
        "\n",
        "Step 1: Data Preparation and Handling Missing Values\n",
        "\n",
        "\n",
        "Load and Explore the Data: Start by loading the dataset (e.g., via pandas.read_csv()). Perform exploratory data analysis (EDA) to understand the structure: check data types (df.dtypes), shape (df.shape), summary statistics (df.describe()), and missing values (df.isnull().sum()). Visualize distributions (e.g., histograms for numerical, bar plots for categorical) and correlations (e.g., heatmap with seaborn) to identify patterns or issues like class imbalance.\n",
        "\n",
        "Identify Missing Values: Quantify missingness per column (e.g., percentage: (df.isnull().sum() / len(df)) * 100). In healthcare, missing values might arise from incomplete records or non-applicable tests.\n",
        "\n",
        "Imputation Strategy:\n",
        "Numerical Features (e.g., age, blood pressure): Use median imputation to handle outliers/skewness (via SimpleImputer(strategy='median') from sklearn). If domain knowledge suggests patterns (e.g., missing lab results correlate with severity), consider advanced methods like KNN imputation (KNNImputer).\n",
        "Categorical Features (e.g., symptoms, family history): Use mode imputation (most frequent value) for low-missingness columns (SimpleImputer(strategy='most_frequent')). For high-missingness (>20%), create a new \"Unknown\" category to avoid introducing bias.\n",
        "Global Handling: Apply imputation separately to train and test sets to prevent data leakage. If missingness is >50% in a column, consider dropping it after consulting domain experts (e.g., doctors) to ensure it doesn't discard valuable info.\n",
        "\n",
        "\n",
        "Post-Imputation Check: Re-run EDA to verify no new issues (e.g., no all-missing rows). Handle any outliers via clipping or winsorization.\n",
        "Additional Preprocessing: Split the data early into train (80%) and test (20%) sets using train_test_split(random_state=42) for reproducibility. If imbalanced, stratify the split.\n",
        "\n",
        "\n",
        "Step 2: Encoding Categorical Features\n",
        "\n",
        "Identify Categorical Features: From EDA, separate numerical (e.g., age, cholesterol) and categorical (e.g., gender, smoking status) columns. Check cardinality (unique values per column) to plan encoding.\n",
        "\n",
        "\n",
        "Encoding Strategy:\n",
        "Nominal (Unordered) Categoricals (e.g., ethnicity, comorbidities): Use One-Hot Encoding to create binary columns (pd.get_dummies() or OneHotEncoder(drop='first') from sklearn). This avoids implying false order and works well with Decision Trees, which handle binary splits naturally. For high-cardinality (e.g., >10 categories), consider target encoding (mean target value per category) to reduce dimensionality, but validate to prevent leakage.\n",
        "\n",
        "Ordinal (Ordered) Categoricals (e.g., disease severity: mild/moderate/severe): Use OrdinalEncoder (OrdinalEncoder()), assigning integer values based on order (e.g., 0=mild, 1=moderate).\n",
        "\n",
        "Binary Categoricals (e.g., yes/no): Map to 0/1 directly (df['feature'].map({'No': 0, 'Yes': 1})).\n",
        "Pipeline Integration: Wrap encoding in a ColumnTransformer from sklearn to apply different transformers to numerical/categorical subsets. Fit the transformer on train data only, then transform both train and test to avoid leakage.\n",
        "\n",
        "\n",
        "Post-Encoding Check: Ensure all features are numerical (Decision Trees in sklearn require this). Scale numerical features if needed (though not strictly necessary for trees, it can help in pipelines).\n",
        "\n",
        "\n",
        "Step 3: Train a Decision Tree Model\n",
        "Model Selection: Use DecisionTreeClassifier from sklearn for binary classification (target: 0=no disease, 1=disease). Start with defaults: criterion='gini' (impurity measure), no depth limit initially.\n",
        "\n",
        "Handle Imbalance (if present): If disease cases are rare (<20%),\n",
        " use class weights (class_weight='balanced') or oversample with SMOTE (SMOTE from imbalanced-learn) on the train set only.\n",
        "Training: Fit the model on the preprocessed train data: model.fit(X_train, y_train). Use a pipeline (Pipeline from sklearn) to chain imputation, encoding, and the tree for reproducibility.\n",
        "Initial Fit: Train a baseline model without tuning to get a quick sense of performance (e.g., via accuracy_score on a validation split).\n",
        "\n",
        "\n",
        "Step 4: Tune Hyperparameters\n",
        "\n",
        "Select Key Hyperparameters: Focus on those controlling overfitting and complexity:\n",
        "max_depth: [3, 5, 7, 10, None] (limits tree depth).\n",
        "min_samples_split: [2, 5, 10, 20] (minimum samples to split a node).\n",
        "min_samples_leaf: [1, 5, 10] (minimum samples per leaf).\n",
        "criterion: ['gini', 'entropy'] (impurity measures).\n",
        "Tuning Method: Use GridSearchCV (exhaustive) or RandomizedSearchCV (faster for large grids) with 5-fold cross-validation (cv=5) on the train set. Set scoring='f1' (or 'recall' for prioritizing true positives in healthcare) and n_jobs=-1 for parallelization.\n",
        "Example: grid_search = GridSearchCV(estimator=DecisionTreeClassifier(random_state=42), param_grid=param_grid, cv=5, scoring='f1').\n",
        "Fit: grid_search.fit(X_train, y_train).\n",
        "Refinement: After initial search, refine around top candidates (e.g., Bayesian optimization with hyperopt if grid is too large). Retrain the best model (grid_search.best_estimator_) on the full train set.\n",
        "Stopping Criteria: Monitor for diminishing returns; aim for a balance between bias and variance.\n",
        "\n",
        "Step 5: Evaluate Model Performance\n",
        "\n",
        "Test Set Evaluation: Predict on the unseen test set: y_pred = best_model.predict(X_test).\n",
        "Key Metrics (beyond accuracy, which can mislead with imbalance):\n",
        "Precision, Recall, F1-Score: Use classification_report from sklearn. High recall is critical to minimize false negatives (missing diseased patients).\n",
        "ROC-AUC: Plot ROC curve (roc_auc_score) to assess discrimination (aim for >0.8).\n",
        "Confusion Matrix: Visualize with confusion_matrix and seaborn heatmap to spot errors (e.g., false positives/negatives).\n",
        "Cross-Validation Score: Report mean CV score from tuning for robustness.\n",
        "Interpretability: Visualize the tree (plot_tree from sklearn) and review feature importances (model.feature_importances_) to explain predictions (e.g., \"High blood pressure >140 splits patients into high-risk\").\n",
        "Bias/Fairness Check: Stratify evaluations by subgroups (e.g., age, gender) using metrics like disparate impact. If bias detected, retrain with balanced sampling.\n",
        "Validation: If data allows, use a hold-out validation set or k-fold on the full dataset. Compare to baselines (e.g., logistic regression) to ensure added value.\n",
        "Iteration: If performance is poor (e.g., F1 <0.7), revisit preprocessing or try ensembles (e.g., Random Forest)."
      ],
      "metadata": {
        "id": "BwSy1871WC6L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "xzRwwl2zWCrR"
      }
    }
  ]
}