{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: What is the role of filters and feature maps in Convolutional Neural Network (CNN)?\n",
        "\n",
        "In Convolutional Neural Networks (CNNs), filters (also called kernels) and feature maps are core components that enable the network to learn and extract spatial features from input data like images.\n",
        "\n",
        "Filters: These are small, learnable matrices (e.g., 3x3 or 5x5) that slide over the input image or previous feature map, performing element-wise multiplications and summations to detect patterns such as edges, textures, or shapes. Each filter learns to recognize specific features during training via backpropagation. For example, an edge-detecting filter might have weights that emphasize differences between adjacent pixels. Filters promote parameter sharing, reducing the model's complexity compared to fully connected layers—evidence from architectures like LeNet-5 shows this leads to efficient learning of hierarchical features.\n",
        "\n",
        "Feature Maps: These are the outputs of applying a filter to the input. Each filter produces one feature map, which is a 2D array highlighting where the filter's pattern is present in the input. Multiple filters generate multiple feature maps (e.g., 32 filters on a 28x28 input might yield 32 feature maps). They capture activations at different spatial locations, allowing the network to build representations from low-level (e.g., edges) to high-level (e.g., objects) features. In practice, feature maps from deeper layers become more abstract, as seen in AlexNet's layers, where early maps detect edges and later ones recognize complex shapes, improving classification accuracy on datasets like ImageNet.\n",
        "\n",
        "Together, they make CNNs translation-invariant and computationally efficient, with empirical studies (e.g., in Zeiler & Fergus, 2014) visualizing feature maps to confirm their role in feature extraction.\n",
        "\n",
        "Question 2: Explain the concepts of padding and stride in CNNs. How do they affect the output dimensions of feature maps?\n",
        "\n",
        "Padding and stride are hyperparameters in convolutional layers that control how filters are applied to inputs, directly influencing the spatial dimensions of output feature maps.\n",
        "\n",
        "Padding: This adds extra pixels (usually zeros) around the input image or feature map to preserve spatial information and prevent shrinking. For example, \"same\" padding ensures the output size matches the input, while \"valid\" padding (no padding) allows shrinking. Padding helps maintain border features, as filters can center on edge pixels.\n",
        "\n",
        "Stride: This defines how many pixels the filter moves (slides) at each step (e.g., stride 1 moves one pixel, stride 2 skips every other). Larger strides reduce the output size faster, introducing subsampling.\n",
        "\n",
        "Effect on Output Dimensions: For an input of height H, width W, filter size F, padding P, and stride S, the output dimensions are:\n",
        "\n",
        "Output Height:\n",
        "\n",
        "Output Width:\n",
        "\n",
        "For instance, a 28x28 input with a 3x3 filter, stride 1, and padding 1 yields 28x28 output (preserving size). Without padding (P=0), it becomes 26x26. Larger strides (e.g., S=2) halve dimensions, reducing parameters but potentially losing detail. This is crucial for controlling receptive fields and computational cost, as seen in VGG nets using stride 1 for fine-grained features and pooling for downsampling.\n",
        "\n",
        "Question 3: Define receptive field in the context of CNNs. Why is it important for deep architectures?\n",
        "\n",
        "The receptive field of a neuron in a CNN is the region of the input image that influences its activation. It grows with depth: for a neuron in layer L, it's determined by the filter sizes and strides of preceding layers. For example, in a simple CNN with 3x3 filters and stride 1, a neuron in layer 3 has a 7x7 receptive field.\n",
        "\n",
        "It's important for deep architectures because larger receptive fields allow neurons to capture global context (e.g., entire objects), enabling better feature integration. Without sufficient size, networks struggle with complex patterns, leading to poor performance—as evidenced by ResNet's design, where deeper layers have expansive fields for tasks like ImageNet classification. However, overly large fields can increase parameters; techniques like dilated convolutions expand fields without more layers.\n",
        "\n",
        "Question 4: Discuss how filter size and stride influence the number of parameters in a CNN.\n",
        "\n",
        "Filter size and stride affect parameter count by determining connectivity and output size, impacting model complexity and efficiency.\n",
        "\n",
        "Filter Size (F): Larger filters (e.g., 7x7 vs. 3x3) increase parameters per layer, as each filter has F×F weights (plus bias). For C input channels and K filters, parameters = K × (F×F × C + 1). Larger F captures broader patterns but risks overfitting; smaller F (e.g., 3x3 in VGG) allows deeper stacks with fewer parameters overall, as shown by VGG-16's ~138M params vs. larger-filter alternatives.\n",
        "\n",
        "Stride (S): Larger strides reduce output feature map size, indirectly lowering parameters in subsequent layers by decreasing input dimensions. For example, stride 2 halves the map, cutting params in the next conv layer. This trades resolution for efficiency, as in AlexNet's pooling-equivalent strides, reducing compute while maintaining performance on ImageNet.\n",
        "\n",
        "Overall, smaller filters and moderate strides minimize parameters (e.g., MobileNet uses 1x1 and 3x3 for efficiency), but optimal choices balance expressiveness and overfitting, per empirical tuning in papers like He et al. (2016).\n",
        "\n",
        "Question 5: Compare and contrast different CNN-based architectures like LeNet, AlexNet, and VGG in terms of depth, filter sizes, and performance.\n",
        "\n",
        "LeNet, AlexNet, and VGG are foundational CNNs, evolving in depth, filter sizes, and performance for image classification.\n",
        "\n",
        "Depth: LeNet-5 has 5 layers (2 conv, 3 FC). AlexNet has 8 layers (5 conv, 3 FC), deeper due to GPUs. VGG-16/19 has 16-19 layers (13-16 conv, 3 FC), enabling hierarchical learning but risking vanishing gradients—ResNet addressed this later.\n",
        "\n",
        "Filter Sizes: LeNet uses 5x5 filters. AlexNet mixes 11x11 (first layer) and 5x5/3x3, with large initial filters for broad features. VGG standardizes 3x3 filters throughout, stacking them for effective receptive fields (e.g., two 3x3 = 5x5 coverage), reducing params.\n",
        "\n",
        "Performance: LeNet achieved ~99% on MNIST (1998). AlexNet dropped ImageNet top-5 error to 15.3% (2012), using ReLU and dropout. VGG improved to 6.8% (2014) with depth, but slower training. VGG excels in transfer learning; AlexNet pioneered GPU training; LeNet laid basics. All outperform shallow nets, but VGG's uniformity aids fine-tuning, per Simonyan et al. (2014)."
      ],
      "metadata": {
        "id": "aKTNY98I4f48"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 6: Using Keras, build and train a simple CNN model on the MNIST dataset from scratch. Include code for module creation, compilation, training, and evaluation."
      ],
      "metadata": {
        "id": "faGbwy775fTo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "\n",
        "# Load the MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "\n",
        "# Preprocess the data\n",
        "# Normalize pixel values to [0, 1]\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "\n",
        "# Reshape to add channel dimension (grayscale images)\n",
        "x_train = x_train.reshape((x_train.shape[0], 28, 28, 1))\n",
        "x_test = x_test.reshape((x_test.shape[0], 28, 28, 1))\n",
        "\n",
        "# Convert labels to categorical (one-hot encoding)\n",
        "y_train = keras.utils.to_categorical(y_train, 10)\n",
        "y_test = keras.utils.to_categorical(y_test, 10)\n",
        "\n",
        "# Build the CNN model\n",
        "model = keras.Sequential([\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(x_train, y_train, epochs=5, batch_size=64, validation_split=0.1)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_loss, test_accuracy = model.evaluate(x_test, y_test)\n",
        "print(f\"Test accuracy: {test_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "7E_KZHT26IgB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c9f6969-9c12-4fd3-9be8-9d627d9da370"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 54ms/step - accuracy: 0.8706 - loss: 0.4285 - val_accuracy: 0.9812 - val_loss: 0.0663\n",
            "Epoch 2/5\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 54ms/step - accuracy: 0.9820 - loss: 0.0593 - val_accuracy: 0.9873 - val_loss: 0.0451\n",
            "Epoch 3/5\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 62ms/step - accuracy: 0.9888 - loss: 0.0373 - val_accuracy: 0.9845 - val_loss: 0.0537\n",
            "Epoch 4/5\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 53ms/step - accuracy: 0.9907 - loss: 0.0294 - val_accuracy: 0.9873 - val_loss: 0.0445\n",
            "Epoch 5/5\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 53ms/step - accuracy: 0.9938 - loss: 0.0204 - val_accuracy: 0.9890 - val_loss: 0.0395\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.9865 - loss: 0.0367\n",
            "Test accuracy: 0.9894\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Question 7: Load and preprocess the CIFAR-10 dataset using Keras, and create a\n",
        "CNN model to classify RGB images. Show your preprocessing and architecture."
      ],
      "metadata": {
        "id": "eeoGBKkCRh8I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "\n",
        "# Load the CIFAR-10 dataset\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
        "\n",
        "# Preprocess the data\n",
        "# Normalize pixel values to [0, 1]\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "\n",
        "# Convert labels to categorical (one-hot encoding)\n",
        "y_train = keras.utils.to_categorical(y_train, 10)\n",
        "y_test = keras.utils.to_categorical(y_test, 10)\n",
        "\n",
        "# Build the CNN model for RGB images (3 channels)\n",
        "model = keras.Sequential([\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(x_train, y_train, epochs=10, batch_size=64, validation_split=0.1)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_loss, test_accuracy = model.evaluate(x_test, y_test)\n",
        "print(f\"Test accuracy: {test_accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S4Ad4vviRnhK",
        "outputId": "a3329bf9-46cc-4b4f-e9dc-249accfbfd13"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "\u001b[1m170498071/170498071\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 0us/step\n",
            "Epoch 1/10\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 98ms/step - accuracy: 0.3202 - loss: 1.8317 - val_accuracy: 0.5018 - val_loss: 1.3646\n",
            "Epoch 2/10\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 92ms/step - accuracy: 0.5439 - loss: 1.2833 - val_accuracy: 0.5990 - val_loss: 1.1352\n",
            "Epoch 3/10\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 91ms/step - accuracy: 0.6133 - loss: 1.1052 - val_accuracy: 0.6322 - val_loss: 1.0498\n",
            "Epoch 4/10\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 92ms/step - accuracy: 0.6569 - loss: 0.9768 - val_accuracy: 0.6428 - val_loss: 1.0458\n",
            "Epoch 5/10\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 90ms/step - accuracy: 0.6889 - loss: 0.8964 - val_accuracy: 0.6900 - val_loss: 0.9098\n",
            "Epoch 6/10\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 93ms/step - accuracy: 0.7114 - loss: 0.8277 - val_accuracy: 0.6888 - val_loss: 0.9268\n",
            "Epoch 7/10\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 91ms/step - accuracy: 0.7405 - loss: 0.7474 - val_accuracy: 0.6952 - val_loss: 0.9128\n",
            "Epoch 8/10\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 92ms/step - accuracy: 0.7546 - loss: 0.7010 - val_accuracy: 0.7022 - val_loss: 0.8844\n",
            "Epoch 9/10\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 90ms/step - accuracy: 0.7686 - loss: 0.6631 - val_accuracy: 0.7110 - val_loss: 0.8660\n",
            "Epoch 10/10\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 89ms/step - accuracy: 0.7870 - loss: 0.6057 - val_accuracy: 0.7218 - val_loss: 0.8718\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - accuracy: 0.7015 - loss: 0.8839\n",
            "Test accuracy: 0.6977\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8: Using PyTorch, write a script to define and train a CNN on the MNIST\n",
        "dataset. Include model definition, data loaders, training loop, and accuracy evaluation.\n"
      ],
      "metadata": {
        "id": "7t6m5TNERn_Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Define the CNN model\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(self.relu(self.conv1(x)))\n",
        "        x = self.pool(self.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 64 * 7 * 7)\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Data preprocessing and loaders\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# Model, loss, and optimizer\n",
        "model = SimpleCNN()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 5\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for images, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader):.4f}')\n",
        "\n",
        "# Evaluation\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "accuracy = 100 * correct / total\n",
        "print(f'Accuracy on test set: {accuracy:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iAr5Hu70RsI_",
        "outputId": "1d14e951-b1a3-4460-89b5-da9864aefd2e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:03<00:00, 2.87MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 158kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:01<00:00, 1.51MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 7.52MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5, Loss: 0.2529\n",
            "Epoch 2/5, Loss: 0.0895\n",
            "Epoch 3/5, Loss: 0.0666\n",
            "Epoch 4/5, Loss: 0.0540\n",
            "Epoch 5/5, Loss: 0.0456\n",
            "Accuracy on test set: 99.15%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9: Given a custom image dataset stored in a local directory, write code using\n",
        "Keras ImageDataGenerator to preprocess and train a CNN model."
      ],
      "metadata": {
        "id": "nxCUEkmGRsuW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "# Define paths (replace with your actual paths)\n",
        "train_dir = './data/train'  # Example: relative path to your training dataset directory\n",
        "validation_dir = './data/validation'  # Example: relative path to your validation dataset directory (or None if using split)\n",
        "\n",
        "# --- Start of fix: Create dummy directories and files for demonstration ---\n",
        "# This section creates a minimal directory structure with dummy images\n",
        "# so that flow_from_directory can run without FileNotFoundError.\n",
        "# You should remove or comment out this section when using your actual dataset.\n",
        "\n",
        "def create_dummy_image(filepath, size=(150, 150)):\n",
        "    image = Image.fromarray(np.random.randint(0, 255, (size[0], size[1], 3), dtype=np.uint8))\n",
        "    image.save(filepath)\n",
        "\n",
        "# Create base data directory if it doesn't exist\n",
        "os.makedirs('./data', exist_ok=True)\n",
        "\n",
        "# Create train and validation directories\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(validation_dir, exist_ok=True)\n",
        "\n",
        "# Define dummy classes\n",
        "dummy_classes = ['class_a', 'class_b']\n",
        "\n",
        "# Create class subdirectories and dummy images in train_dir\n",
        "for class_name in dummy_classes:\n",
        "    class_path = os.path.join(train_dir, class_name)\n",
        "    os.makedirs(class_path, exist_ok=True)\n",
        "    for i in range(5):  # Create 5 dummy images per class\n",
        "        create_dummy_image(os.path.join(class_path, f'train_image_{class_name}_{i}.png'))\n",
        "\n",
        "# Create class subdirectories and dummy images in validation_dir\n",
        "for class_name in dummy_classes:\n",
        "    class_path = os.path.join(validation_dir, class_name)\n",
        "    os.makedirs(class_path, exist_ok=True)\n",
        "    for i in range(2):  # Create 2 dummy images per class\n",
        "        create_dummy_image(os.path.join(class_path, f'val_image_{class_name}_{i}.png'))\n",
        "\n",
        "print(\"Dummy directories and images created for demonstration.\")\n",
        "\n",
        "# --- End of fix ---\n",
        "\n",
        "\n",
        "# ImageDataGenerator for preprocessing and augmentation\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,  # Normalize pixel values to [0, 1]\n",
        "    rotation_range=20,  # Random rotations\n",
        "    width_shift_range=0.2,  # Random horizontal shifts\n",
        "    height_shift_range=0.2,  # Random vertical shifts\n",
        "    shear_range=0.2,  # Random shearing\n",
        "    zoom_range=0.2,  # Random zoom\n",
        "    horizontal_flip=True,  # Random horizontal flips\n",
        "    fill_mode='nearest',  # Fill mode for new pixels\n",
        "    validation_split=0.2  # Optional: use 20% of training data for validation if no separate dir\n",
        ")\n",
        "\n",
        "validation_datagen = ImageDataGenerator(rescale=1./255)  # Only rescale for validation\n",
        "\n",
        "# Data loaders\n",
        "if validation_dir and os.path.exists(validation_dir):\n",
        "    # Use separate validation directory\n",
        "    train_generator = train_datagen.flow_from_directory(\n",
        "        train_dir,\n",
        "        target_size=(150, 150),  # Resize images to 150x150\n",
        "        batch_size=32,\n",
        "        class_mode='categorical'  # For multi-class; use 'binary' for binary\n",
        "    )\n",
        "    validation_generator = validation_datagen.flow_from_directory(\n",
        "        validation_dir,\n",
        "        target_size=(150, 150),\n",
        "        batch_size=32,\n",
        "        class_mode='categorical'\n",
        "    )\n",
        "else:\n",
        "    # Use validation split from training data\n",
        "    train_generator = train_datagen.flow_from_directory(\n",
        "        train_dir,\n",
        "        target_size=(150, 150),\n",
        "        batch_size=32,\n",
        "        class_mode='categorical',\n",
        "        subset='training'  # Use for training\n",
        "    )\n",
        "    validation_generator = train_datagen.flow_from_directory(\n",
        "        train_dir,\n",
        "        target_size=(150, 150),\n",
        "        batch_size=32,\n",
        "        class_mode='categorical',\n",
        "        subset='validation'  # Use for validation\n",
        "    )\n",
        "\n",
        "# Build the CNN model\n",
        "model = keras.Sequential([\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(512, activation='relu'),\n",
        "    layers.Dense(len(train_generator.class_indices), activation='softmax')  # Number of classes\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=train_generator.samples // train_generator.batch_size,\n",
        "    epochs=10,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=validation_generator.samples // validation_generator.batch_size\n",
        ")\n",
        "\n",
        "# Evaluate on validation set\n",
        "val_loss, val_accuracy = model.evaluate(validation_generator)\n",
        "print(f\"Validation accuracy: {val_accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1MGo_Zz0Rv79",
        "outputId": "beb6996e-ccff-4986-ad13-495a6aebc716"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dummy directories and images created for demonstration.\n",
            "Found 10 images belonging to 2 classes.\n",
            "Found 4 images belonging to 2 classes.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step - accuracy: 0.5000 - loss: 0.6926 - val_accuracy: 0.5000 - val_loss: 8.0946\n",
            "Epoch 2/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 848ms/step - accuracy: 0.5000 - loss: 6.5292 - val_accuracy: 0.5000 - val_loss: 1.1573\n",
            "Epoch 3/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 811ms/step - accuracy: 0.5000 - loss: 0.9758 - val_accuracy: 0.5000 - val_loss: 0.9181\n",
            "Epoch 4/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - accuracy: 0.5000 - loss: 0.8222 - val_accuracy: 0.5000 - val_loss: 0.6901\n",
            "Epoch 5/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 0.5000 - loss: 0.6912 - val_accuracy: 0.5000 - val_loss: 0.7252\n",
            "Epoch 6/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 0.5000 - loss: 0.7103 - val_accuracy: 0.5000 - val_loss: 0.6942\n",
            "Epoch 7/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - accuracy: 0.5000 - loss: 0.6922 - val_accuracy: 0.5000 - val_loss: 0.6944\n",
            "Epoch 8/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - accuracy: 0.5000 - loss: 0.6949 - val_accuracy: 0.5000 - val_loss: 0.7001\n",
            "Epoch 9/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - accuracy: 0.5000 - loss: 0.6962 - val_accuracy: 0.5000 - val_loss: 0.6947\n",
            "Epoch 10/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 801ms/step - accuracy: 0.5000 - loss: 0.6938 - val_accuracy: 0.5000 - val_loss: 0.6984\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - accuracy: 0.5000 - loss: 0.6984\n",
            "Validation accuracy: 0.5000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: You are working on a web application for a medical imaging startup. Your\n",
        "task is to build and deploy a CNN model that classifies chest X-ray images into “Normal”\n",
        "and “Pneumonia” categories. Describe your end-to-end approach–from data preparation\n",
        "and model training to deploying the model as a web app using Streamlit."
      ],
      "metadata": {
        "id": "eoosNxXGRwQR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Data Preparation\n",
        "Dataset Acquisition: Download the Chest X-Ray Images dataset (e.g., from Kaggle). It includes ~5,800 training images and ~1,600 test images, split into \"NORMAL\" and \"PNEUMONIA\" folders. Use libraries like os and shutil to organize data into directories.\n",
        "Preprocessing:\n",
        "Resize images to a consistent size (e.g., 224x224 pixels) using tf.image.resize to standardize input for the CNN.\n",
        "Normalize pixel values to [0, 1] by dividing by 255.\n",
        "Apply data augmentation (e.g., random rotations, flips) using ImageDataGenerator to prevent overfitting and improve generalization, especially since pneumonia images may be imbalanced.\n",
        "Split data: 80% training, 10% validation, 10% test. Use train_test_split or Keras' flow_from_directory for efficient loading.\n",
        "Handling Imbalances: If pneumonia cases dominate, use class weights during training (e.g., via class_weight in model.fit).\n",
        "2. Model Building and Training\n",
        "Architecture: Build a simple CNN using Keras for binary classification:\n",
        "Input: 224x224x3 RGB images.\n",
        "Layers: Conv2D (32 filters, 3x3 kernel, ReLU), MaxPooling2D (2x2), Conv2D (64 filters), MaxPooling2D, Conv2D (128 filters), MaxPooling2D, Flatten, Dense (128 units, ReLU), Dense (1 unit, Sigmoid) for binary output.\n",
        "This is lightweight (~1M parameters) and effective for image classification tasks like this.\n",
        "Compilation: Use Adam optimizer, binary cross-entropy loss, and accuracy/metrics like AUC-ROC for evaluation.\n",
        "Training: Train for 10-20 epochs with batch size 32, using early stopping (monitor validation loss) to avoid overfitting. Expect ~85-95% accuracy on test data, based on similar models in literature (e.g., studies on this dataset achieve high performance with CNNs).\n",
        "Evaluation: Assess with confusion matrix, precision, recall, and F1-score. Visualize training curves using Matplotlib to ensure convergence.\n",
        "\n",
        "3. Model Deployment as a Web App Using Streamlit\n",
        "Save the Model: After training, save the model as an HDF5 file (model.save('chest_xray_model.h5')) for easy loading.\n",
        "Build the Streamlit App:\n",
        "Install Streamlit (pip install streamlit).\n",
        "Create app.py: Load the model, add a file uploader for X-ray images, preprocess uploaded images (resize/normalize), make predictions, and display results (e.g., \"Normal\" or \"Pneumonia\" with confidence score).\n",
        "Include UI elements like titles, descriptions, and warnings (e.g., \"This is not a substitute for medical advice\").\n",
        "Deployment:\n",
        "Run locally with streamlit run app.py.\n",
        "For production, deploy on platforms like Streamlit Cloud, Heroku, or AWS. Upload the model file and ensure dependencies (e.g., TensorFlow) are specified in requirements.txt.\n",
        "Add security: Limit file types to images, validate inputs, and consider authentication for sensitive data.\n",
        "Testing and Iteration: Test the app with sample images, gather feedback, and retrain the model periodically with new data for robustness."
      ],
      "metadata": {
        "id": "C_8xAMvGSOHN"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qQiT5rIpR2EZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}